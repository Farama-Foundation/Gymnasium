{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Action Masking in the Taxi Environment\n\nThis tutorial demonstrates how to use action masking in the Taxi environment to improve reinforcement learning performance by preventing invalid actions.\n\nThe Taxi environment is a classic grid world problem where a taxi needs to pick up a passenger and drop them off at their destination. In this environment, not all actions are valid at every state - for example, you can't drive through walls or pick up a passenger when you're not at their location.\n\nAction masking is a technique that helps reinforcement learning agents avoid selecting invalid actions by providing a binary mask that indicates which actions are valid in the current state. This can significantly improve learning efficiency and performance.\n\n## Understanding the Taxi Environment\n\nThe Taxi environment has 6 possible actions:\n\n- 0: Move south (down)\n- 1: Move north (up)\n- 2: Move east (right)\n- 3: Move west (left)\n- 4: Pickup passenger\n- 5: Drop off passenger\n\nThe environment provides an ``action_mask`` in the info dictionary returned by ``reset()`` and ``step()``.\nThis mask is a binary array where 1 indicates a valid action and 0 indicates an invalid action.\n\n## How Action Masking Works\n\nAction masking works by constraining the agent's action selection to only valid actions:\n\n1. During exploration: When selecting random actions, we only choose from the set of valid actions\n2. During exploitation: When selecting the best action based on Q-values, we only consider Q-values for valid actions\n3. During Q-learning updates: We compute the maximum future Q-value only over valid actions in the next state\n\nLet's implement this step by step:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport gymnasium as gym\n\n\n# Base random seed for reproducibility\nBASE_RANDOM_SEED = 58922320\n\n\ndef train_q_learning(\n    env,\n    use_action_mask: bool = True,\n    episodes: int = 5000,\n    seed: int = BASE_RANDOM_SEED,\n    learning_rate: float = 0.1,\n    discount_factor: float = 0.95,\n    epsilon: float = 0.1,\n) -> dict:\n    \"\"\"Train a Q-learning agent with or without action masking.\"\"\"\n    # Set random seeds for reproducibility\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Initialize Q-table\n    n_states = env.observation_space.n\n    n_actions = env.action_space.n\n    q_table = np.zeros((n_states, n_actions))\n\n    # Track episode rewards for analysis\n    episode_rewards = []\n\n    for episode in range(episodes):\n        # Reset environment\n        state, info = env.reset(seed=seed + episode)\n        total_reward = 0\n        done = False\n        truncated = False\n\n        while not (done or truncated):\n            # Get action mask if using it\n            action_mask = info[\"action_mask\"] if use_action_mask else None\n\n            # Epsilon-greedy action selection with masking\n            if np.random.random() < epsilon:\n                # Random action selection\n                if use_action_mask:\n                    # Only select from valid actions\n                    valid_actions = np.nonzero(action_mask == 1)[0]\n                    action = np.random.choice(valid_actions)\n                else:\n                    # Select from all actions\n                    action = np.random.randint(0, n_actions)\n            else:\n                # Greedy action selection\n                if use_action_mask:\n                    # Only consider valid actions for exploitation\n                    valid_actions = np.nonzero(action_mask == 1)[0]\n                    if len(valid_actions) > 0:\n                        action = valid_actions[np.argmax(q_table[state, valid_actions])]\n                    else:\n                        action = np.random.randint(0, n_actions)\n                else:\n                    # Consider all actions\n                    action = np.argmax(q_table[state])\n\n            # Take action and observe result\n            next_state, reward, done, truncated, info = env.step(action)\n            total_reward += reward\n\n            # Q-learning update\n            if not (done or truncated):\n                if use_action_mask:\n                    # Only consider valid next actions for bootstrapping\n                    next_mask = info[\"action_mask\"]\n                    valid_next_actions = np.nonzero(next_mask == 1)[0]\n                    if len(valid_next_actions) > 0:\n                        next_max = np.max(q_table[next_state, valid_next_actions])\n                    else:\n                        next_max = 0\n                else:\n                    # Consider all next actions\n                    next_max = np.max(q_table[next_state])\n\n                # Update Q-value\n                q_table[state, action] = q_table[state, action] + learning_rate * (\n                    reward + discount_factor * next_max - q_table[state, action]\n                )\n\n            state = next_state\n\n        episode_rewards.append(total_reward)\n\n    return {\n        \"episode_rewards\": episode_rewards,\n        \"mean_reward\": np.mean(episode_rewards),\n        \"std_reward\": np.std(episode_rewards),\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running the Experiment\n\nNow we'll run experiments to compare the performance of Q-learning agents with and without action masking.\nWe'll use multiple random seeds to ensure robust statistical comparison.\n\nThe experiment setup:\n- 12 independent runs with different random seeds\n- 5000 episodes per run\n- Standard Q-learning hyperparameters ``(\u03b1=0.1, \u03b3=0.95, \u03b5=0.1)``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Experiment parameters\nn_runs = 12\nepisodes = 5000\nlearning_rate = 0.1\ndiscount_factor = 0.95\nepsilon = 0.1\n\n# Generate different seeds for each run\nseeds = [BASE_RANDOM_SEED + i for i in range(n_runs)]\n\n# Store results for comparison\nmasked_results_list = []\nunmasked_results_list = []\n\n# Run experiments with different seeds\nfor i, seed in enumerate(seeds):\n    print(f\"Run {i + 1}/{n_runs} with seed {seed}\")\n\n    # Train agent WITH action masking\n    env_masked = gym.make(\"Taxi-v3\")\n    masked_results = train_q_learning(\n        env_masked,\n        use_action_mask=True,\n        seed=seed,\n        learning_rate=learning_rate,\n        discount_factor=discount_factor,\n        epsilon=epsilon,\n        episodes=episodes,\n    )\n    env_masked.close()\n    masked_results_list.append(masked_results)\n\n    # Train agent WITHOUT action masking\n    env_unmasked = gym.make(\"Taxi-v3\")\n    unmasked_results = train_q_learning(\n        env_unmasked,\n        use_action_mask=False,\n        seed=seed,\n        learning_rate=learning_rate,\n        discount_factor=discount_factor,\n        epsilon=epsilon,\n        episodes=episodes,\n    )\n    env_unmasked.close()\n    unmasked_results_list.append(unmasked_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing Results\n\nAfter running all experiments, we calculate statistics and create visualizations to compare the performance of both approaches.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate statistics across runs\nmasked_mean_rewards = [r[\"mean_reward\"] for r in masked_results_list]\nunmasked_mean_rewards = [r[\"mean_reward\"] for r in unmasked_results_list]\n\nmasked_overall_mean = np.mean(masked_mean_rewards)\nmasked_overall_std = np.std(masked_mean_rewards)\nunmasked_overall_mean = np.mean(unmasked_mean_rewards)\nunmasked_overall_std = np.std(unmasked_mean_rewards)\n\n# Create visualization\nplt.figure(figsize=(12, 8), dpi=100)\n\n# Plot individual runs with low alpha\nfor i, (masked_results, unmasked_results) in enumerate(\n    zip(masked_results_list, unmasked_results_list)\n):\n    plt.plot(\n        masked_results[\"episode_rewards\"],\n        label=\"With Action Masking\" if i == 0 else None,\n        color=\"blue\",\n        alpha=0.1,\n    )\n    plt.plot(\n        unmasked_results[\"episode_rewards\"],\n        label=\"Without Action Masking\" if i == 0 else None,\n        color=\"red\",\n        alpha=0.1,\n    )\n\n# Calculate and plot mean curves across all runs\nmasked_mean_curve = np.mean([r[\"episode_rewards\"] for r in masked_results_list], axis=0)\nunmasked_mean_curve = np.mean(\n    [r[\"episode_rewards\"] for r in unmasked_results_list], axis=0\n)\n\nplt.plot(\n    masked_mean_curve, label=\"With Action Masking (Mean)\", color=\"blue\", linewidth=2\n)\nplt.plot(\n    unmasked_mean_curve,\n    label=\"Without Action Masking (Mean)\",\n    color=\"red\",\n    linewidth=2,\n)\n\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total Reward\")\nplt.title(\"Training Performance: Q-Learning with vs without Action Masking\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Save the figure\nsavefig_folder = Path(\"_static/img/tutorials/\")\nsavefig_folder.mkdir(parents=True, exist_ok=True)\nplt.savefig(\n    savefig_folder / \"taxi_v3_action_masking_comparison.png\",\n    bbox_inches=\"tight\",\n    dpi=150,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://_static/img/tutorials/taxi_v3_action_masking_comparison.png\">\n\n# Results Analysis\n\nThe comparison demonstrates several important benefits of using action masking:\n\nKey Benefits of Action Masking:\n\n1. Faster Convergence: Agents with action masking typically learn faster because they don't waste time exploring\ninvalid actions.\n\n2. Better Performance: By focusing only on valid actions, the agent can achieve higher rewards more consistently.\n\n3. More Stable Learning: Action masking reduces the variance in learning by eliminating the randomness associated with invalid action selection.\n\n4. Practical Applicability: In real-world scenarios, action masking prevents agents from taking actions that could be dangerous or impossible.\n\nReminder of Key Implementation Details\n\n- Action Selection: We filter available actions using ``np.nonzero(action_mask == 1)[0]`` to get only valid actions\n- Q-Value Updates: When computing the maximum future Q-value, we only consider valid  actions in the next state\n- Exploration: Random action selection is constrained to the set of valid actions\n\nThis approach ensures that the agent never selects invalid actions while still maintaining the exploration-exploitation balance necessary for effective learning.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}