{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Make your own custom environment\n\nThis tutorial shows how to create new environment and links to relevant useful wrappers, utilities and tests included in Gymnasium.\n\n## Setup\n\n### Recommended solution\n\n1. Install ``pipx`` following the [pipx documentation](https://pypa.github.io/pipx/installation/).\n2. Then install Copier:\n\n.. code:: console\n\n    pipx install copier\n\n### Alternative solutions\n\nInstall Copier with Pip or Conda:\n\n.. code:: console\n\n    pip install copier\n\nor\n\n.. code:: console\n\n    conda install -c conda-forge copier\n\n\n## Generate your environment\n\nYou can check that ``Copier`` has been correctly installed by running the following command, which should output a version number:\n\n.. code:: console\n\n    copier --version\n\nThen you can just run the following command and replace the string ``path/to/directory`` by the path to the directory where you want to create your new project.\n\n.. code:: console\n\n    copier copy https://github.com/Farama-Foundation/gymnasium-env-template.git \"path/to/directory\"\n\nAnswer the questions, and when it's finished you should get a project structure like the following:\n\n.. code:: sh\n\n    .\n    \u251c\u2500\u2500 gymnasium_env\n    \u2502        \u251c\u2500\u2500 envs\n    \u2502        \u2502       \u251c\u2500\u2500 grid_world.py\n    \u2502        \u2502       \u2514\u2500\u2500 __init__.py\n    \u2502        \u251c\u2500\u2500 __init__.py\n    \u2502        \u2514\u2500\u2500 wrappers\n    \u2502            \u251c\u2500\u2500 clip_reward.py\n    \u2502            \u251c\u2500\u2500 discrete_actions.py\n    \u2502            \u251c\u2500\u2500 __init__.py\n    \u2502            \u251c\u2500\u2500 reacher_weighted_reward.py\n    \u2502            \u2514\u2500\u2500 relative_position.py\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 pyproject.toml\n    \u2514\u2500\u2500 README.md\n\n## Subclassing gymnasium.Env\n\nBefore learning how to create your own environment you should check out\n[the documentation of Gymnasium\u2019s API](/api/env)_.\n\nTo illustrate the process of subclassing ``gymnasium.Env``, we will\nimplement a very simplistic game, called ``GridWorldEnv``. We will write\nthe code for our custom environment in\n``gymnasium_env/envs/grid_world.py``. The environment\nconsists of a 2-dimensional square grid of fixed size (specified via the\n``size`` parameter during construction). The agent can move vertically\nor horizontally between grid cells in each timestep. The goal of the\nagent is to navigate to a target on the grid that has been placed\nrandomly at the beginning of the episode.\n\n-  Observations provide the location of the target and agent.\n-  There are 4 actions in our environment, corresponding to the\n   movements \u201cright\u201d, \u201cup\u201d, \u201cleft\u201d, and \u201cdown\u201d.\n-  A done signal is issued as soon as the agent has navigated to the\n   grid cell where the target is located.\n-  Rewards are binary and sparse, meaning that the immediate reward is\n   always zero, unless the agent has reached the target, then it is 1.\n\nAn episode in this environment (with ``size=5``) might look like this:\n\n .. image:: /_static/videos/tutorials/environment-creation-example-episode.gif\n    :width: 400\n    :alt: Example episode of the custom environment\n\nwhere the blue dot is the agent and the red square represents the\ntarget.\n\nLet us look at the source code of ``GridWorldEnv`` piece by piece:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Declaration and Initialization\n\nOur custom environment will inherit from the abstract class\n``gymnasium.Env``. You shouldn\u2019t forget to add the ``metadata``\nattribute to your class. There, you should specify the render-modes that\nare supported by your environment (e.g., ``\"human\"``, ``\"rgb_array\"``,\n``\"ansi\"``) and the framerate at which your environment should be\nrendered. Every environment should support ``None`` as render-mode; you\ndon\u2019t need to add it in the metadata. In ``GridWorldEnv``, we will\nsupport the modes \u201crgb_array\u201d and \u201chuman\u201d and render at 4 FPS.\n\nThe ``__init__`` method of our environment will accept the integer\n``size``, that determines the size of the square grid. We will set up\nsome variables for rendering and define ``self.observation_space`` and\n``self.action_space``. In our case, observations should provide\ninformation about the location of the agent and target on the\n2-dimensional grid. We will choose to represent observations in the form\nof dictionaries with keys ``\"agent\"`` and ``\"target\"``. An observation\nmay look like ``{\"agent\": array([1, 0]), \"target\": array([0, 3])}``.\nSince we have 4 actions in our environment (\u201cright\u201d, \u201cup\u201d, \u201cleft\u201d,\n\u201cdown\u201d), we will use ``Discrete(4)`` as an action space. Here is the\ndeclaration of ``GridWorldEnv`` and the implementation of ``__init__``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# gymnasium_env/envs/grid_world.py\nfrom enum import Enum\n\nimport numpy as np\nimport pygame\n\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass Actions(Enum):\n    RIGHT = 0\n    UP = 1\n    LEFT = 2\n    DOWN = 3\n\n\nclass GridWorldEnv(gym.Env):\n    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n\n    def __init__(self, render_mode=None, size=5):\n        self.size = size  # The size of the square grid\n        self.window_size = 512  # The size of the PyGame window\n\n        # Observations are dictionaries with the agent's and the target's location.\n        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n        self.observation_space = spaces.Dict(\n            {\n                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n            }\n        )\n        self._agent_location = np.array([-1, -1], dtype=int)\n        self._target_location = np.array([-1, -1], dtype=int)\n\n        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n        self.action_space = spaces.Discrete(4)\n\n        \"\"\"\n        The following dictionary maps abstract actions from `self.action_space` to\n        the direction we will walk in if that action is taken.\n        i.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n        \"\"\"\n        self._action_to_direction = {\n            Actions.RIGHT.value: np.array([1, 0]),\n            Actions.UP.value: np.array([0, 1]),\n            Actions.LEFT.value: np.array([-1, 0]),\n            Actions.DOWN.value: np.array([0, -1]),\n        }\n\n        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n        self.render_mode = render_mode\n\n        \"\"\"\n        If human-rendering is used, `self.window` will be a reference\n        to the window that we draw to. `self.clock` will be a clock that is used\n        to ensure that the environment is rendered at the correct framerate in\n        human-mode. They will remain `None` until human-mode is used for the\n        first time.\n        \"\"\"\n        self.window = None\n        self.clock = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constructing Observations From Environment States\n\nSince we will need to compute observations both in ``reset`` and\n``step``, it is often convenient to have a (private) method ``_get_obs``\nthat translates the environment\u2019s state into an observation. However,\nthis is not mandatory and you may as well compute observations in\n``reset`` and ``step`` separately:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _get_obs(self):\n        return {\"agent\": self._agent_location, \"target\": self._target_location}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also implement a similar method for the auxiliary information\nthat is returned by ``step`` and ``reset``. In our case, we would like\nto provide the manhattan distance between the agent and the target:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _get_info(self):\n        return {\n            \"distance\": np.linalg.norm(\n                self._agent_location - self._target_location, ord=1\n            )\n        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oftentimes, info will also contain some data that is only available\ninside the ``step`` method (e.g., individual reward terms). In that case,\nwe would have to update the dictionary that is returned by ``_get_info``\nin ``step``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reset\n\nThe ``reset`` method will be called to initiate a new episode. You may\nassume that the ``step`` method will not be called before ``reset`` has\nbeen called. Moreover, ``reset`` should be called whenever a done signal\nhas been issued. Users may pass the ``seed`` keyword to ``reset`` to\ninitialize any random number generator that is used by the environment\nto a deterministic state. It is recommended to use the random number\ngenerator ``self.np_random`` that is provided by the environment\u2019s base\nclass, ``gymnasium.Env``. If you only use this RNG, you do not need to\nworry much about seeding, *but you need to remember to call\n``super().reset(seed=seed)``* to make sure that ``gymnasium.Env``\ncorrectly seeds the RNG. Once this is done, we can randomly set the\nstate of our environment. In our case, we randomly choose the agent\u2019s\nlocation and the random sample target positions, until it does not\ncoincide with the agent\u2019s position.\n\nThe ``reset`` method should return a tuple of the initial observation\nand some auxiliary information. We can use the methods ``_get_obs`` and\n``_get_info`` that we implemented earlier for that:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def reset(self, seed=None, options=None):\n        # We need the following line to seed self.np_random\n        super().reset(seed=seed)\n\n        # Choose the agent's location uniformly at random\n        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n\n        # We will sample the target's location randomly until it does not coincide with the agent's location\n        self._target_location = self._agent_location\n        while np.array_equal(self._target_location, self._agent_location):\n            self._target_location = self.np_random.integers(\n                0, self.size, size=2, dtype=int\n            )\n\n        observation = self._get_obs()\n        info = self._get_info()\n\n        if self.render_mode == \"human\":\n            self._render_frame()\n\n        return observation, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step\n\nThe ``step`` method usually contains most of the logic of your\nenvironment. It accepts an ``action``, computes the state of the\nenvironment after applying that action and returns the 5-tuple\n``(observation, reward, terminated, truncated, info)``. See\n:meth:`gymnasium.Env.step`. Once the new state of the environment has\nbeen computed, we can check whether it is a terminal state and we set\n``done`` accordingly. Since we are using sparse binary rewards in\n``GridWorldEnv``, computing ``reward`` is trivial once we know\n``done``.To gather ``observation`` and ``info``, we can again make\nuse of ``_get_obs`` and ``_get_info``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def step(self, action):\n        # Map the action (element of {0,1,2,3}) to the direction we walk in\n        direction = self._action_to_direction[action]\n        # We use `np.clip` to make sure we don't leave the grid\n        self._agent_location = np.clip(\n            self._agent_location + direction, 0, self.size - 1\n        )\n        # An episode is done iff the agent has reached the target\n        terminated = np.array_equal(self._agent_location, self._target_location)\n        reward = 1 if terminated else 0  # Binary sparse rewards\n        observation = self._get_obs()\n        info = self._get_info()\n\n        if self.render_mode == \"human\":\n            self._render_frame()\n\n        return observation, reward, terminated, False, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rendering\n\nHere, we are using PyGame for rendering. A similar approach to rendering\nis used in many environments that are included with Gymnasium and you\ncan use it as a skeleton for your own environments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def render(self):\n        if self.render_mode == \"rgb_array\":\n            return self._render_frame()\n\n    def _render_frame(self):\n        if self.window is None and self.render_mode == \"human\":\n            pygame.init()\n            pygame.display.init()\n            self.window = pygame.display.set_mode(\n                (self.window_size, self.window_size)\n            )\n        if self.clock is None and self.render_mode == \"human\":\n            self.clock = pygame.time.Clock()\n\n        canvas = pygame.Surface((self.window_size, self.window_size))\n        canvas.fill((255, 255, 255))\n        pix_square_size = (\n            self.window_size / self.size\n        )  # The size of a single grid square in pixels\n\n        # First we draw the target\n        pygame.draw.rect(\n            canvas,\n            (255, 0, 0),\n            pygame.Rect(\n                pix_square_size * self._target_location,\n                (pix_square_size, pix_square_size),\n            ),\n        )\n        # Now we draw the agent\n        pygame.draw.circle(\n            canvas,\n            (0, 0, 255),\n            (self._agent_location + 0.5) * pix_square_size,\n            pix_square_size / 3,\n        )\n\n        # Finally, add some gridlines\n        for x in range(self.size + 1):\n            pygame.draw.line(\n                canvas,\n                0,\n                (0, pix_square_size * x),\n                (self.window_size, pix_square_size * x),\n                width=3,\n            )\n            pygame.draw.line(\n                canvas,\n                0,\n                (pix_square_size * x, 0),\n                (pix_square_size * x, self.window_size),\n                width=3,\n            )\n\n        if self.render_mode == \"human\":\n            # The following line copies our drawings from `canvas` to the visible window\n            self.window.blit(canvas, canvas.get_rect())\n            pygame.event.pump()\n            pygame.display.update()\n\n            # We need to ensure that human-rendering occurs at the predefined framerate.\n            # The following line will automatically add a delay to keep the framerate stable.\n            self.clock.tick(self.metadata[\"render_fps\"])\n        else:  # rgb_array\n            return np.transpose(\n                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Close\n\nThe ``close`` method should close any open resources that were used by\nthe environment. In many cases, you don\u2019t actually have to bother to\nimplement this method. However, in our example ``render_mode`` may be\n``\"human\"`` and we might need to close the window that has been opened:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def close(self):\n        if self.window is not None:\n            pygame.display.quit()\n            pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In other environments ``close`` might also close files that were opened\nor release other resources. You shouldn\u2019t interact with the environment\nafter having called ``close``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Registering Envs\n\nIn order for the custom environments to be detected by Gymnasium, they\nmust be registered as follows. We will choose to put this code in\n``gymnasium_env/__init__.py``.\n\n.. code:: python\n\n   from gymnasium.envs.registration import register\n\n   register(\n       id=\"gymnasium_env/GridWorld-v0\",\n       entry_point=\"gymnasium_env.envs:GridWorldEnv\",\n   )\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The environment ID consists of three components, two of which are\noptional: an optional namespace (here: ``gymnasium_env``), a mandatory\nname (here: ``GridWorld``) and an optional but recommended version\n(here: v0). It might have also been registered as ``GridWorld-v0`` (the\nrecommended approach), ``GridWorld`` or ``gymnasium_env/GridWorld``, and\nthe appropriate ID should then be used during environment creation.\n\nThe keyword argument ``max_episode_steps=300`` will ensure that\nGridWorld environments that are instantiated via ``gymnasium.make`` will\nbe wrapped in a ``TimeLimit`` wrapper (see [the wrapper\ndocumentation](/api/wrappers)_ for more information). A done signal\nwill then be produced if the agent has reached the target *or* 300 steps\nhave been executed in the current episode. To distinguish truncation and\ntermination, you can check ``info[\"TimeLimit.truncated\"]``.\n\nApart from ``id`` and ``entrypoint``, you may pass the following\nadditional keyword arguments to ``register``:\n\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n| Name                 | Type      | Default   | Description                                                                                                   |\n+======================+===========+===========+===============================================================================================================+\n| ``reward_threshold`` | ``float`` | ``None``  | The reward threshold before the task is  considered solved                                                    |\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n| ``nondeterministic`` | ``bool``  | ``False`` | Whether this environment is non-deterministic even after seeding                                              |\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n| ``max_episode_steps``| ``int``   | ``None``  | The maximum number of steps that an episode can consist of. If not ``None``, a ``TimeLimit`` wrapper is added |\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n| ``order_enforce``    | ``bool``  | ``True``  | Whether to wrap the environment in an  ``OrderEnforcing`` wrapper                                             |\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n| ``kwargs``           | ``dict``  | ``{}``    | The default kwargs to pass to the environment class                                                           |\n+----------------------+-----------+-----------+---------------------------------------------------------------------------------------------------------------+\n\nMost of these keywords (except for ``max_episode_steps``,\n``order_enforce`` and ``kwargs``) do not alter the behavior of\nenvironment instances but merely provide some extra information about\nyour environment. After registration, our custom ``GridWorldEnv``\nenvironment can be created with\n``env = gymnasium.make('gymnasium_env/GridWorld-v0')``.\n\n``gymnasium_env/envs/__init__.py`` should have:\n\n.. code:: python\n\n   from gymnasium_env.envs.grid_world import GridWorldEnv\n\nIf your environment is not registered, you may optionally pass a module\nto import, that would register your environment before creating it like\nthis - ``env = gymnasium.make('module:Env-v0')``, where ``module``\ncontains the registration code. For the GridWorld env, the registration\ncode is run by importing ``gymnasium_env`` so if it were not possible to\nimport gymnasium_env explicitly, you could register while making by\n``env = gymnasium.make('gymnasium_env:gymnasium_env/GridWorld-v0')``. This\nis especially useful when you\u2019re allowed to pass only the environment ID\ninto a third-party codebase (eg. learning library). This lets you\nregister your environment without needing to edit the library\u2019s source\ncode.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Package\n\nThe last step is to structure our code as a Python package. This\ninvolves configuring ``pyproject.toml``. A minimal example of how\nto do so is as follows:\n\n.. code:: toml\n\n   [build-system]\n   requires = [\"hatchling\"]\n   build-backend = \"hatchling.build\"\n\n   [project]\n   name = \"gymnasium_env\"\n   version = \"0.0.1\"\n   dependencies = [\n     \"gymnasium\",\n     \"pygame==2.1.3\",\n     \"pre-commit\",\n   ]\n\n## Creating Environment Instances\n\nNow you can install your package locally with:\n\n.. code:: console\n\n   pip install -e .\n\nAnd you can create an instance of the environment via:\n\n.. code:: python\n\n   # run_gymnasium_env.py\n\n   import gymnasium\n   import gymnasium_env\n   env = gymnasium.make('gymnasium_env/GridWorld-v0')\n\nYou can also pass keyword arguments of your environment\u2019s constructor to\n``gymnasium.make`` to customize the environment. In our case, we could\ndo:\n\n.. code:: python\n\n   env = gymnasium.make('gymnasium_env/GridWorld-v0', size=10)\n\nSometimes, you may find it more convenient to skip registration and call\nthe environment\u2019s constructor yourself. Some may find this approach more\npythonic and environments that are instantiated like this are also\nperfectly fine (but remember to add wrappers as well!).\n\n## Using Wrappers\n\nOftentimes, we want to use different variants of a custom environment,\nor we want to modify the behavior of an environment that is provided by\nGymnasium or some other party. Wrappers allow us to do this without\nchanging the environment implementation or adding any boilerplate code.\nCheck out the [wrapper documentation](/api/wrappers/)_ for details on\nhow to use wrappers and instructions for implementing your own. In our\nexample, observations cannot be used directly in learning code because\nthey are dictionaries. However, we don\u2019t actually need to touch our\nenvironment implementation to fix this! We can simply add a wrapper on\ntop of environment instances to flatten observations into a single\narray:\n\n.. code:: python\n\n   import gymnasium\n   import gymnasium_env\n   from gymnasium.wrappers import FlattenObservation\n\n   env = gymnasium.make('gymnasium_env/GridWorld-v0')\n   wrapped_env = FlattenObservation(env)\n   print(wrapped_env.reset())     # E.g.  [3 0 3 3], {}\n\nWrappers have the big advantage that they make environments highly\nmodular. For instance, instead of flattening the observations from\nGridWorld, you might only want to look at the relative position of the\ntarget and the agent. In the section on\n[ObservationWrappers](/api/wrappers/observation_wrappers/#observation-wrappers)_ we have\nimplemented a wrapper that does this job. This wrapper is also available\nin ``gymnasium_env/wrappers/relative_position.py``:\n\n.. code:: python\n\n   import gymnasium\n   import gymnasium_env\n   from gymnasium_env.wrappers import RelativePosition\n\n   env = gymnasium.make('gymnasium_env/GridWorld-v0')\n   wrapped_env = RelativePosition(env)\n   print(wrapped_env.reset())     # E.g.  [-3  3], {}\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}