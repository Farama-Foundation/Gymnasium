<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark">
    <meta name="description" content="A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)">
    <meta property="og:title" content="Gymnasium Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)" />
    <meta property="og:url" content="https://gymnasium.farama.org/tutorials/training_agents/mujoco_reinforce.html" /><meta property="og:image" content="https://gymnasium.farama.org/_static/img/gymnasium-github.png" /><meta name="twitter:card" content="summary_large_image"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../genindex/" /><link rel="search" title="Search" href="../../../search/" /><link rel="next" title="Speeding up A2C Training with Vector Envs" href="../vector_a2c/" /><link rel="prev" title="Solving Frozenlake with Tabular Q-Learning" href="../frozenlake_q_learning/" />
        <link rel="canonical" href="https://gymnasium.farama.org/tutorials/training_agents/mujoco_reinforce.html" />

    <link rel="shortcut icon" href="../../../_static/favicon.png"/><!-- Generated with Sphinx 7.4.7 and Furo 2023.08.19.dev1 -->
        <title>Training using REINFORCE for Mujoco - Gymnasium Documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?v=3e7f4c72" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?v=82c8b628" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    <header class="farama-header" aria-label="Farama header">
      <div class="farama-header__container">
        <div class="farama-header__left--mobile">
          <label class="nav-overlay-icon" for="__navigation">
            <div class="visually-hidden">Toggle site navigation sidebar</div>
            <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
              <defs></defs>
              <line x1="0.5" y1="4" x2="23.5" y2="4"></line>
              <line x1="0.232" y1="12" x2="23.5" y2="12"></line>
              <line x1="0.232" y1="20" x2="23.5" y2="20"></line>
            </svg>
          </label>
        </div>
        <div class="farama-header__left farama-header__center--mobile">
          <a href="../../../">
              <img class="farama-header__logo only-light" src="../../../_static/img/gymnasium_black.svg" alt="Light Logo"/>
              <img class="farama-header__logo only-dark" src="../../../_static/img/gymnasium_white.svg" alt="Dark Logo"/>
            <span class="farama-header__title">Gymnasium Documentation</span>
          </a>
        </div>
        <div class="farama-header__right">
          <div class="farama-header-menu">
            <button class="farama-header-menu__btn" aria-label="Open Farama Menu" aria-expanded="false" aria-haspopup="true" aria-controls="farama-menu">
              <img class="farama-black-logo-invert" src="../../../_static/img/farama-logo-header.svg">
              <svg viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <polyline style="stroke-linecap: round; stroke-linejoin: round; fill: none; stroke-width: 2px;" points="1 7 12 18 23 7"></polyline>
              </svg>
            </button>
            <div class="farama-header-menu-container farama-hidden" aria-hidden="true" id="farama-menu">
              <div class="farama-header-menu__header">
                <a href="https://farama.org">
                  <img class="farama-header-menu__logo farama-white-logo-invert" src="../../../_static/img/farama_solid_white.svg" alt="Farama Foundation logo">
                  <span>Farama Foundation</span>
                </a>
                <div class="farama-header-menu-header__right">
                  <button id="farama-close-menu">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor"
                      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-close">
                      <line x1="3" y1="21" x2="21" y2="3"></line>
                      <line x1="3" y1="3" x2="21" y2="21"></line>
                    </svg>
                  </button>
                </div>
              </div>
              <div class="farama-header-menu__body">
                <!-- Response from farama.org/api/projects.json -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>

    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<div class="page">
  <!--<header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../"><div class="brand">Gymnasium Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>-->
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="farama-sidebar__title" href="../../../">
      <img class="farama-header__logo only-light" src="../../../_static/img/gymnasium_black.svg" alt="Light Logo"/>
      <img class="farama-header__logo only-dark" src="../../../_static/img/gymnasium_white.svg" alt="Dark Logo"/>
    <span class="farama-header__title">Gymnasium Documentation</span>
  </a><form class="sidebar-search-container" method="get" action="../../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/basic_usage/">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/train_agent/">Training an Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/create_custom_env/">Create a Custom Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/record_agent/">Recording Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/speed_up_env/">Speeding Up Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/migration_guide/">Gym Migration Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/env/">Env</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/registry/">Make and register</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/spaces/">Spaces</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Spaces</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/fundamental/">Fundamental Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/composite/">Composite Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/utils/">Spaces Utils</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/wrappers/">Wrappers</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Wrappers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/table/">List of Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/misc_wrappers/">Misc Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/action_wrappers/">Action Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/observation_wrappers/">Observation Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/reward_wrappers/">Reward Wrappers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/vector/">Vectorize</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Vectorize</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/wrappers/">Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/async_vector_env/">AsyncVectorEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/sync_vector_env/">SyncVectorEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/utils/">Utility functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/">Utility functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/functional/">Functional Env</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environments</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/classic_control/">Classic Control</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Classic Control</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/acrobot/">Acrobot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/cart_pole/">Cart Pole</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/mountain_car_continuous/">Mountain Car Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/mountain_car/">Mountain Car</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/pendulum/">Pendulum</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/box2d/">Box2D</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Box2D</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/bipedal_walker/">Bipedal Walker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/car_racing/">Car Racing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/lunar_lander/">Lunar Lander</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/toy_text/">Toy Text</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Toy Text</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/blackjack/">Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/taxi/">Taxi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/cliff_walking/">Cliff Walking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/frozen_lake/">Frozen Lake</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/mujoco/">MuJoCo</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of MuJoCo</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/ant/">Ant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/half_cheetah/">Half Cheetah</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/hopper/">Hopper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/humanoid/">Humanoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/humanoid_standup/">Humanoid Standup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/inverted_double_pendulum/">Inverted Double Pendulum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/inverted_pendulum/">Inverted Pendulum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/pusher/">Pusher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/reacher/">Reacher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/swimmer/">Swimmer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/walker2d/">Walker2D</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../environments/atari/">Atari</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../environments/third_party_environments/">External Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../gymnasium_basics/">Gymnasium Basics</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Gymnasium Basics</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/environment_creation/">Make your own custom environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/handling_time_limits/">Handling Time Limits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/implementing_custom_wrappers/">Implementing Custom Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/load_quadruped_model/">Load custom quadruped robot environments</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../">Training Agents</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of Training Agents</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/">Action Masking in the Taxi Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#running-the-experiment">Running the Experiment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#visualizing-results">Visualizing Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#results-analysis">Results Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blackjack_q_learning/">Solving Blackjack with Tabular Q-Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../frozenlake_q_learning/">Solving Frozenlake with Tabular Q-Learning</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Training using REINFORCE for Mujoco</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vector_a2c/">Speeding up A2C Training with Vector Envs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../third-party-tutorials/">Third-Party Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium">Github</a></li>
<li class="toctree-l1"><a class="reference external" href="https://arxiv.org/abs/2407.17032">Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gymnasium_release_notes/">Gymnasium Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gym_release_notes/">Gym Release Notes</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium/blob/main/docs/README.md">Contribute to the Docs</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main-container">

    

    

    <div class="main">
      <div class="content">
        <div class="article-container">
          <a href="#" class="back-to-top muted-link">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
              <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
            </svg>
            <span>Back to top</span>
          </a>
          <div class="content-icon-container">
      <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/Farama-Foundation/Gymnasium/edit/main/docs/tutorials/training_agents/mujoco_reinforce.py" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
              <button class="theme-toggle" title="Toggle color theme">
                <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                <svg class="theme-icon-when-auto">
                  <use href="#svg-sun-half"></use>
                </svg>
                <svg class="theme-icon-when-dark">
                  <use href="#svg-moon"></use>
                </svg>
                <svg class="theme-icon-when-light">
                  <use href="#svg-sun"></use>
                </svg>
              </button>
            </div>
            <label class="toc-overlay-icon toc-content-icon" for="__toc">
              <div class="visually-hidden">Toggle table of contents sidebar</div>
              <i class="icon"><svg>
                  <use href="#svg-toc"></use>
                </svg></i>
            </label>
          </div>
          <article role="main">
            
            <div class="sphx-glr-example-title admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial is compatible with Gymnasium version 1.2.1.</p>
</div>
<section id="training-using-reinforce-for-mujoco">
<span id="sphx-glr-tutorials-training-agents-mujoco-reinforce-py"></span><h1>Training using REINFORCE for Mujoco<a class="headerlink" href="#training-using-reinforce-for-mujoco" title="Link to this heading">¶</a></h1>
<a class="reference internal image-reference" href="../../../_images/mujoco_reinforce_fig1.gif"><img alt="agent-environment-diagram" src="../../../_images/mujoco_reinforce_fig1.gif" style="width: 400px;" />
</a>
<p>This tutorial implements REINFORCE with neural networks for a MuJoCo environment.</p>
<p>We will be using <strong>REINFORCE</strong>, one of the earliest policy gradient methods. Unlike going under the burden of learning a value function first and then deriving a policy out of it,
REINFORCE optimizes the policy directly. In other words, it is trained to maximize the probability of Monte-Carlo returns. More on that later.</p>
<p><strong>Inverted Pendulum</strong> is Mujoco’s cartpole but now powered by the Mujoco physics simulator -
which allows more complex experiments (such as varying the effects of gravity).
This environment involves a cart that can moved linearly, with a pole fixed on it at one end and having another end free.
The cart can be pushed left or right, and the goal is to balance the pole on the top of the cart by applying forces on the cart.
More information on the environment could be found at <a class="reference external" href="https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/">https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/</a></p>
<p><strong>Training Objectives</strong>: To balance the pole (inverted pendulum) on top of the cart</p>
<p><strong>Actions</strong>: The agent takes a 1D vector for actions. The action space is a continuous <code class="docutils literal notranslate"><span class="pre">(action)</span></code> in <code class="docutils literal notranslate"><span class="pre">[-3,</span> <span class="pre">3]</span></code>,
where action represents the numerical force applied to the cart
(with magnitude representing the amount of force and sign representing the direction)</p>
<p><strong>Approach</strong>: We use PyTorch to code REINFORCE from scratch to train a Neural Network policy to master Inverted Pendulum.</p>
<p>An explanation of the Gymnasium v0.26+ <cite>Env.step()</cite> function</p>
<p><code class="docutils literal notranslate"><span class="pre">env.step(A)</span></code> allows us to take an action ‘A’ in the current environment ‘env’. The environment then executes the action
and returns five variables:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">next_obs</span></code>: This is the observation that the agent will receive after taking the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reward</span></code>: This is the reward that the agent will receive after taking the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">terminated</span></code>: This is a boolean variable that indicates whether or not the environment has terminated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">truncated</span></code>: This is a boolean variable that also indicates whether the episode ended by early truncation, i.e., a time limit is reached.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">info</span></code>: This is a dictionary that might contain additional information about the environment.</p></li>
</ul>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributions.normal</span><span class="w"> </span><span class="kn">import</span> <span class="n">Normal</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>


<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<section id="policy-network">
<h2>Policy Network<a class="headerlink" href="#policy-network" title="Link to this heading">¶</a></h2>
<img alt="../../../_images/mujoco_reinforce_fig2.png" src="../../../_images/mujoco_reinforce_fig2.png" />
<p>We start by building a policy that the agent will learn using REINFORCE.
A policy is a mapping from the current environment observation to a probability distribution of the actions to be taken.
The policy used in the tutorial is parameterized by a neural network. It consists of 2 linear layers that are shared between both the predicted mean and standard deviation.
Further, the single individual linear layers are used to estimate the mean and the standard deviation. <code class="docutils literal notranslate"><span class="pre">nn.Tanh</span></code> is used as a non-linearity between the hidden layers.
The following function estimates a mean and standard deviation of a normal distribution from which an action is sampled. Hence it is expected for the policy to learn
appropriate weights to output means and standard deviation based on the current observation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Policy_Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parametrized Policy Network.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_space_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes a neural network that estimates the mean and standard deviation</span>
<span class="sd">         of a normal distribution from which an action is sampled from.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space_dims: Dimension of the observation space</span>
<span class="sd">            action_space_dims: Dimension of the action space</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">hidden_space1</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># Nothing special with 16, feel free to change</span>
        <span class="n">hidden_space2</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Nothing special with 32, feel free to change</span>

        <span class="c1"># Shared Network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">obs_space_dims</span><span class="p">,</span> <span class="n">hidden_space1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_space1</span><span class="p">,</span> <span class="n">hidden_space2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="c1"># Policy Mean specific Linear Layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_mean_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_space2</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Policy Std Dev specific Linear Layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_stddev_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_space2</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Conditioned on the observation, returns the mean and standard deviation</span>
<span class="sd">         of a normal distribution from which an action is sampled from.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Observation from the environment</span>

<span class="sd">        Returns:</span>
<span class="sd">            action_means: predicted mean of the normal distribution</span>
<span class="sd">            action_stddevs: predicted standard deviation of the normal distribution</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shared_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_net</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

        <span class="n">action_means</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_mean_net</span><span class="p">(</span><span class="n">shared_features</span><span class="p">)</span>
        <span class="n">action_stddevs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_stddev_net</span><span class="p">(</span><span class="n">shared_features</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">action_means</span><span class="p">,</span> <span class="n">action_stddevs</span>
</pre></div>
</div>
</section>
<section id="building-an-agent">
<h2>Building an agent<a class="headerlink" href="#building-an-agent" title="Link to this heading">¶</a></h2>
<img alt="../../../_images/mujoco_reinforce_fig3.jpeg" src="../../../_images/mujoco_reinforce_fig3.jpeg" />
<p>Now that we are done building the policy, let us develop <strong>REINFORCE</strong> which gives life to the policy network.
The algorithm of REINFORCE could be found above. As mentioned before, REINFORCE aims to maximize the Monte-Carlo returns.</p>
<p>Fun Fact: REINFORCE is an acronym for “ ‘RE’ward ‘I’ncrement ‘N’on-negative ‘F’actor times ‘O’ffset ‘R’einforcement times ‘C’haracteristic ‘E’ligibility</p>
<p>Note: The choice of hyperparameters is to train a decently performing agent. No extensive hyperparameter
tuning was done.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">REINFORCE</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;REINFORCE algorithm.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_space_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an agent that learns a policy via REINFORCE algorithm [1]</span>
<span class="sd">        to solve the task at hand (Inverted Pendulum v4).</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space_dims: Dimension of the observation space</span>
<span class="sd">            action_space_dims: Dimension of the action space</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>  <span class="c1"># Learning rate for policy optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># Discount factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>  <span class="c1"># small number for mathematical stability</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Stores probability values of the sampled action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Stores the corresponding rewards</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">Policy_Network</span><span class="p">(</span><span class="n">obs_space_dims</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns an action, conditioned on the policy and observation.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: Observation from the environment</span>

<span class="sd">        Returns:</span>
<span class="sd">            action: Action to be performed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">state</span><span class="p">]))</span>
        <span class="n">action_means</span><span class="p">,</span> <span class="n">action_stddevs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># create a normal distribution from the predicted</span>
        <span class="c1">#   mean and standard deviation and sample an action</span>
        <span class="n">distrib</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">action_means</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">action_stddevs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">distrib</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">distrib</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates the policy network&#39;s weights.&quot;&quot;&quot;</span>
        <span class="n">running_g</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">gs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Discounted return (backwards) - [::-1] will return an array in reverse</span>
        <span class="k">for</span> <span class="n">R</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">running_g</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">running_g</span>
            <span class="n">gs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">running_g</span><span class="p">)</span>

        <span class="n">deltas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">gs</span><span class="p">)</span>

        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Update the loss with the mean log probability and deltas</span>
        <span class="c1"># Now, we compute the correct total loss by taking the sum of the element-wise products.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">deltas</span><span class="p">)</span>

        <span class="c1"># Update the policy network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Empty / zero out all episode-centric/related variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
<p>Now lets train the policy using REINFORCE to master the task of Inverted Pendulum.</p>
<p>Following is the overview of the training procedure</p>
<blockquote>
<div><dl>
<dt>for seed in random seeds</dt><dd><p>reinitialize agent</p>
<dl>
<dt>for episode in range of max number of episodes</dt><dd><dl>
<dt>until episode is done</dt><dd><p>sample action based on current observation</p>
<p>take action and receive reward and next observation</p>
<p>store action take, its probability, and the observed reward</p>
</dd>
</dl>
<p>update the policy</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>Note: Deep RL is fairly brittle concerning random seed in a lot of common use cases (<a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/spinningup.html">https://spinningup.openai.com/en/latest/spinningup/spinningup.html</a>).
Hence it is important to test out various seeds, which we will be doing.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and wrap the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;InvertedPendulum-v4&quot;</span><span class="p">)</span>
<span class="n">wrapped_env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">RecordEpisodeStatistics</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># Records episode-reward</span>

<span class="n">total_num_episodes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">5e3</span><span class="p">)</span>  <span class="c1"># Total number of episodes</span>
<span class="c1"># Observation-space of InvertedPendulum-v4 (4)</span>
<span class="n">obs_space_dims</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Action-space of InvertedPendulum-v4 (1)</span>
<span class="n">action_space_dims</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">rewards_over_seeds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>  <span class="c1"># Fibonacci seeds</span>
    <span class="c1"># set seed</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Reinitialize agent every seed</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">obs_space_dims</span><span class="p">,</span> <span class="n">action_space_dims</span><span class="p">)</span>
    <span class="n">reward_over_episodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_num_episodes</span><span class="p">):</span>
        <span class="c1"># gymnasium v26 requires users to set seed while resetting the environment</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">wrapped_env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

            <span class="c1"># Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`</span>
            <span class="c1"># These represent the next observation, the reward from the step,</span>
            <span class="c1"># if the episode is terminated, if the episode is truncated and</span>
            <span class="c1"># additional info from the step</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">wrapped_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="c1"># End the episode when either truncated or terminated is true</span>
            <span class="c1">#  - truncated: The episode duration reaches max number of timesteps</span>
            <span class="c1">#  - terminated: Any of the state space values is no longer finite.</span>
            <span class="c1">#</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>

        <span class="n">reward_over_episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wrapped_env</span><span class="o">.</span><span class="n">return_queue</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wrapped_env</span><span class="o">.</span><span class="n">return_queue</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Episode:&quot;</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="s2">&quot;Average Reward:&quot;</span><span class="p">,</span> <span class="n">avg_reward</span><span class="p">)</span>

    <span class="n">rewards_over_seeds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_over_episodes</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plot-learning-curve">
<h2>Plot learning curve<a class="headerlink" href="#plot-learning-curve" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rewards_over_seeds</span><span class="p">)</span><span class="o">.</span><span class="n">melt</span><span class="p">()</span>
<span class="n">df1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;variable&quot;</span><span class="p">:</span> <span class="s2">&quot;episodes&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;reward&quot;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;rainbow&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;episodes&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df1</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;REINFORCE for InvertedPendulum-v4&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/mujoco_reinforce_fig4.png" src="../../../_images/mujoco_reinforce_fig4.png" />
<p>Author: Siddarth Chandrasekar</p>
<p>License: MIT License</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<p>[1] Williams, Ronald J.. “Simple statistical gradient-following
algorithms for connectionist reinforcement learning.” Machine Learning 8
(2004): 229-256.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-training-agents-mujoco-reinforce-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/6095f0c7ad0799958d2dcdf642aeeed7/mujoco_reinforce.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">mujoco_reinforce.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/5bd8b56318388cc6c3585454b47469d9/mujoco_reinforce.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">mujoco_reinforce.ipynb</span></code></a></p>
</div>
</div>
</section>
</section>

          </article>
        </div>
        <footer>
          
          <div class="related-pages">
            <a class="next-page" href="../vector_a2c/">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Speeding up A2C Training with Vector Envs</div>
              </div>
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
            </a>
            <a class="prev-page" href="../frozenlake_q_learning/">
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Solving Frozenlake with Tabular Q-Learning</div>
                
              </div>
            </a>
          </div>
          <div class="bottom-of-page">
            <div class="left-details">
              <div class="copyright">
                Copyright &#169; 2025 Farama Foundation
              </div>
              <!--
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            -->
            </div>
            <div class="right-details">
              <div class="icons">
                <a class="muted-link" href="https://github.com/Farama-Foundation/Gymnasium/"
                  aria-label="On GitHub">
                  <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd"
                      d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z">
                    </path>
                  </svg>
                </a>
              </div>
            </div>
          </div>
          
        </footer>
      </div>
      <aside class="toc-drawer">
        
        
        <div class="toc-sticky toc-scroll">
          <div class="toc-title-container">
            <span class="toc-title">
              On this page
            </span>
          </div>
          <div class="toc-tree-container">
            <div class="toc-tree">
              <ul>
<li><a class="reference internal" href="#">Training using REINFORCE for Mujoco</a><ul>
<li><a class="reference internal" href="#policy-network">Policy Network</a></li>
<li><a class="reference internal" href="#building-an-agent">Building an agent</a></li>
<li><a class="reference internal" href="#plot-learning-curve">Plot learning curve</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
        
        
      </aside>
    </div>
  </div>
</div>
    <script>
      const toggleMenu = () => {
        const menuBtn = document.querySelector(".farama-header-menu__btn");
        const menuContainer = document.querySelector(".farama-header-menu-container");
        if (document.querySelector(".farama-header-menu").classList.contains("active")) {
          menuBtn.setAttribute("aria-expanded", "false");
          menuContainer.setAttribute("aria-hidden", "true");
        } else {
          menuBtn.setAttribute("aria-expanded", "true");
          menuContainer.setAttribute("aria-hidden", "false");
        }
        document.querySelector(".farama-header-menu").classList.toggle("active");
      }

      document.querySelector(".farama-header-menu__btn").addEventListener("click", toggleMenu);
      document.getElementById("farama-close-menu").addEventListener("click", toggleMenu);
    </script>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6H9C8TWXZ8"></script>
      <script>
        const enableGtag = () => {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-6H9C8TWXZ8');
        }
        (() => {
            if (!localStorage.getItem("acceptedCookieAlert")) {
                const boxElem = document.createElement("div");
                boxElem.classList.add("cookie-alert");
                const containerElem = document.createElement("div");
                containerElem.classList.add("cookie-alert__container");
                const textElem = document.createElement("p");
                textElem.innerHTML = `This page uses <a href="https://analytics.google.com/">
                                    Google Analytics</a> to collect statistics.`;
                                    containerElem.appendChild(textElem);

                const declineBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Deny",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__decline",
                  }
                );
                declineBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", false);
                  boxElem.remove();
                });

                const acceptBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Allow",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__accept",
                  }
                );
                acceptBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", true);
                  boxElem.remove();
                  enableGtag();
                });

                containerElem.appendChild(declineBtn);
                containerElem.appendChild(acceptBtn);
                boxElem.appendChild(containerElem);
                document.body.appendChild(boxElem);
            } else if (localStorage.getItem("acceptedCookieAlert") === "true") {
              enableGtag();
            }
        })()
      </script>

    <script src="../../../_static/documentation_options.js?v=e86ceb7b"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/furo.js?v=7660844c"></script>
    
    <script>

      const createProjectsList = (projects, displayImages) => {
        const ulElem = Object.assign(document.createElement('ul'),
          {
            className:'farama-header-menu-list',
          }
        )
        for (let project of projects) {
          const liElem = document.createElement("li");
          const aElem = Object.assign(document.createElement("a"),
            {
              href: project.link
            }
          );
          liElem.appendChild(aElem);
          if (displayImages) {
            const imgElem = Object.assign(document.createElement("img"),
              {
                src: project.image ? imagesBasepath + project.image : imagesBasepath + "/farama_black.svg",
                alt: `${project.name} logo`,
                className: "farama-black-logo-invert"
              }
            );
            aElem.appendChild(imgElem);
          }
          aElem.appendChild(document.createTextNode(project.name));
          ulElem.appendChild(liElem);
        }
        return ulElem;
      }

      // Create menu with Farama projects by using the API at farama.org/api/projects.json
      const createCORSRequest = (method, url) => {
        let xhr = new XMLHttpRequest();
        xhr.responseType = 'json';

        if ("withCredentials" in xhr) {
          xhr.open(method, url, true);
        } else if (typeof XDomainRequest != "undefined") {
          // IE8 & IE9
          xhr = new XDomainRequest();
          xhr.open(method, url);
        } else {
          // CORS not supported.
          xhr = null;
        }
        return xhr;
      };

      const url = 'https://farama.org/api/projects.json';
      const imagesBasepath = "https://farama.org/assets/images"
      const method = 'GET';
      let xhr = createCORSRequest(method, url);

      xhr.onload = () => {
        const jsonResponse = xhr.response;
        const sections = {
          "Core Projects": [],
          "Mature Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Incubating Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Foundation": [
            {
              name: "About",
              link: "https://farama.org/about"
            },
            {
              name: "Standards",
              link: "https://farama.org/project_standards",
            },
            {
              name: "Donate",
              link: "https://farama.org/donations"
            }
          ]
        }

        // Categorize projects
        Object.keys(jsonResponse).forEach(key => {
          projectJson = jsonResponse[key];
          if (projectJson.website !== null) {
            projectJson.link = projectJson.website;
          } else {
            projectJson.link = projectJson.github;
          }
          if (projectJson.type === "core") {
            sections["Core Projects"].push(projectJson)
          } else if (projectJson.type == "mature") {
            if (projectJson.website !== null) {
              sections["Mature Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Mature Projects"]["Repositories"].push(projectJson)
            }
          } else {
            if (projectJson.website !== null) {
              sections["Incubating Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Incubating Projects"]["Repositories"].push(projectJson)
            }
          }
        })

        const menuContainer = document.querySelector(".farama-header-menu__body");

        Object.keys(sections).forEach((key, i) => {
          const sectionElem = Object.assign(
            document.createElement('div'), {
              className:'farama-header-menu__section',
            }
          )
          sectionElem.appendChild(Object.assign(document.createElement('span'),
            {
              className:'farama-header-menu__section-title' ,
              innerText: key
            }
          ))
          // is not a list
          if (sections[key].constructor !== Array) {
            const subSections = sections[key];
            const subSectionContainerElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsections-container',
                  style: 'display: flex'
                }
            )
            Object.keys(subSections).forEach((subKey, i) => {
              const subSectionElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsection',
                }
              )
              subSectionElem.appendChild(Object.assign(document.createElement('span'),
                {
                  className:'farama-header-menu__subsection-title' ,
                  innerText: subKey
                }
              ))
              const ulElem = createProjectsList(subSections[subKey], key !== 'Foundation');
              subSectionElem.appendChild(ulElem);
              subSectionContainerElem.appendChild(subSectionElem);
            })
            sectionElem.appendChild(subSectionContainerElem);
          } else {
            const projects = sections[key];
            const ulElem = createProjectsList(projects, true);
            sectionElem.appendChild(ulElem);
          }
          menuContainer.appendChild(sectionElem)
        });
      }

      xhr.onerror = function() {
        console.error("Unable to load projects");
      };

      xhr.send();
    </script>

    
    <script>
      const versioningConfig = {
        githubUser: 'Farama-Foundation',
        githubRepo: 'Gymnasium',
      };
      fetch('/main/_static/versioning/versioning_menu.html').then(response => {
        if (response.status === 200) {
            response.text().then(text => {
                const container = document.createElement("div");
                container.innerHTML = text;
                document.querySelector("body").appendChild(container);
                // innerHtml doenst evaluate scripts, we need to add them dynamically
                Array.from(container.querySelectorAll("script")).forEach(oldScript => {
                    const newScript = document.createElement("script");
                    Array.from(oldScript.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value));
                    newScript.appendChild(document.createTextNode(oldScript.innerHTML));
                    oldScript.parentNode.replaceChild(newScript, oldScript);
                });
            });
        } else {
            console.warn("Unable to load versioning menu", response);
        }
      });
    </script>

    </body>
</html>