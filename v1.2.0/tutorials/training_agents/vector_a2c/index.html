<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark">
    <meta name="description" content="A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)">
    <meta property="og:title" content="Gymnasium Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)" />
    <meta property="og:url" content="https://gymnasium.farama.org/tutorials/training_agents/vector_a2c.html" /><meta property="og:image" content="https://gymnasium.farama.org/_static/img/gymnasium-github.png" /><meta name="twitter:card" content="summary_large_image"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../genindex/" /><link rel="search" title="Search" href="../../../search/" /><link rel="next" title="Third-Party Tutorials" href="../../third-party-tutorials/" /><link rel="prev" title="Training using REINFORCE for Mujoco" href="../mujoco_reinforce/" />
        <link rel="canonical" href="https://gymnasium.farama.org/tutorials/training_agents/vector_a2c.html" />

    <link rel="shortcut icon" href="../../../_static/favicon.png"/><!-- Generated with Sphinx 7.4.7 and Furo 2023.08.19.dev1 -->
        <title>Speeding up A2C Training with Vector Envs - Gymnasium Documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?v=3e7f4c72" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?v=82c8b628" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    <header class="farama-header" aria-label="Farama header">
      <div class="farama-header__container">
        <div class="farama-header__left--mobile">
          <label class="nav-overlay-icon" for="__navigation">
            <div class="visually-hidden">Toggle site navigation sidebar</div>
            <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
              <defs></defs>
              <line x1="0.5" y1="4" x2="23.5" y2="4"></line>
              <line x1="0.232" y1="12" x2="23.5" y2="12"></line>
              <line x1="0.232" y1="20" x2="23.5" y2="20"></line>
            </svg>
          </label>
        </div>
        <div class="farama-header__left farama-header__center--mobile">
          <a href="../../../">
              <img class="farama-header__logo only-light" src="../../../_static/img/gymnasium_black.svg" alt="Light Logo"/>
              <img class="farama-header__logo only-dark" src="../../../_static/img/gymnasium_white.svg" alt="Dark Logo"/>
            <span class="farama-header__title">Gymnasium Documentation</span>
          </a>
        </div>
        <div class="farama-header__right">
          <div class="farama-header-menu">
            <button class="farama-header-menu__btn" aria-label="Open Farama Menu" aria-expanded="false" aria-haspopup="true" aria-controls="farama-menu">
              <img class="farama-black-logo-invert" src="../../../_static/img/farama-logo-header.svg">
              <svg viewBox="0 0 24 24" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <polyline style="stroke-linecap: round; stroke-linejoin: round; fill: none; stroke-width: 2px;" points="1 7 12 18 23 7"></polyline>
              </svg>
            </button>
            <div class="farama-header-menu-container farama-hidden" aria-hidden="true" id="farama-menu">
              <div class="farama-header-menu__header">
                <a href="https://farama.org">
                  <img class="farama-header-menu__logo farama-white-logo-invert" src="../../../_static/img/farama_solid_white.svg" alt="Farama Foundation logo">
                  <span>Farama Foundation</span>
                </a>
                <div class="farama-header-menu-header__right">
                  <button id="farama-close-menu">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor"
                      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon-close">
                      <line x1="3" y1="21" x2="21" y2="3"></line>
                      <line x1="3" y1="3" x2="21" y2="21"></line>
                    </svg>
                  </button>
                </div>
              </div>
              <div class="farama-header-menu__body">
                <!-- Response from farama.org/api/projects.json -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>

    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<div class="page">
  <!--<header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../"><div class="brand">Gymnasium Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>-->
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="farama-sidebar__title" href="../../../">
      <img class="farama-header__logo only-light" src="../../../_static/img/gymnasium_black.svg" alt="Light Logo"/>
      <img class="farama-header__logo only-dark" src="../../../_static/img/gymnasium_white.svg" alt="Dark Logo"/>
    <span class="farama-header__title">Gymnasium Documentation</span>
  </a><form class="sidebar-search-container" method="get" action="../../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/basic_usage/">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/train_agent/">Training an Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/create_custom_env/">Create a Custom Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/record_agent/">Recording Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/speed_up_env/">Speeding Up Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction/migration_guide/">Gym Migration Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/env/">Env</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/registry/">Make and register</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/spaces/">Spaces</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Spaces</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/fundamental/">Fundamental Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/composite/">Composite Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/spaces/utils/">Spaces Utils</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/wrappers/">Wrappers</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Wrappers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/table/">List of Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/misc_wrappers/">Misc Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/action_wrappers/">Action Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/observation_wrappers/">Observation Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/wrappers/reward_wrappers/">Reward Wrappers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../api/vector/">Vectorize</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Vectorize</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/wrappers/">Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/async_vector_env/">AsyncVectorEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/sync_vector_env/">SyncVectorEnv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/vector/utils/">Utility functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/">Utility functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/functional/">Functional Env</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environments</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/classic_control/">Classic Control</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Classic Control</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/acrobot/">Acrobot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/cart_pole/">Cart Pole</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/mountain_car_continuous/">Mountain Car Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/mountain_car/">Mountain Car</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/classic_control/pendulum/">Pendulum</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/box2d/">Box2D</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Box2D</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/bipedal_walker/">Bipedal Walker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/car_racing/">Car Racing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/box2d/lunar_lander/">Lunar Lander</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/toy_text/">Toy Text</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle navigation of Toy Text</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/blackjack/">Blackjack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/taxi/">Taxi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/cliff_walking/">Cliff Walking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/toy_text/frozen_lake/">Frozen Lake</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../environments/mujoco/">MuJoCo</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle navigation of MuJoCo</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/ant/">Ant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/half_cheetah/">Half Cheetah</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/hopper/">Hopper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/humanoid/">Humanoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/humanoid_standup/">Humanoid Standup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/inverted_double_pendulum/">Inverted Double Pendulum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/inverted_pendulum/">Inverted Pendulum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/pusher/">Pusher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/reacher/">Reacher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/swimmer/">Swimmer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../environments/mujoco/walker2d/">Walker2D</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../environments/atari/">Atari</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../environments/third_party_environments/">External Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../gymnasium_basics/">Gymnasium Basics</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle navigation of Gymnasium Basics</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/environment_creation/">Make your own custom environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/handling_time_limits/">Handling Time Limits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/implementing_custom_wrappers/">Implementing Custom Wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gymnasium_basics/load_quadruped_model/">Load custom quadruped robot environments</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../">Training Agents</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle navigation of Training Agents</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/">Action Masking in the Taxi Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#running-the-experiment">Running the Experiment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#visualizing-results">Visualizing Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="../action_masking_taxi/#results-analysis">Results Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blackjack_q_learning/">Solving Blackjack with Tabular Q-Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../frozenlake_q_learning/">Solving Frozenlake with Tabular Q-Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mujoco_reinforce/">Training using REINFORCE for Mujoco</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Speeding up A2C Training with Vector Envs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../third-party-tutorials/">Third-Party Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium">Github</a></li>
<li class="toctree-l1"><a class="reference external" href="https://arxiv.org/abs/2407.17032">Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gymnasium_release_notes/">Gymnasium Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gym_release_notes/">Gym Release Notes</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Farama-Foundation/Gymnasium/blob/main/docs/README.md">Contribute to the Docs</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main-container">

    

    

    <div class="main">
      <div class="content">
        <div class="article-container">
          <a href="#" class="back-to-top muted-link">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
              <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
            </svg>
            <span>Back to top</span>
          </a>
          <div class="content-icon-container">
      <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/Farama-Foundation/Gymnasium/edit/main/docs/tutorials/training_agents/vector_a2c.py" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
              <button class="theme-toggle" title="Toggle color theme">
                <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                <svg class="theme-icon-when-auto">
                  <use href="#svg-sun-half"></use>
                </svg>
                <svg class="theme-icon-when-dark">
                  <use href="#svg-moon"></use>
                </svg>
                <svg class="theme-icon-when-light">
                  <use href="#svg-sun"></use>
                </svg>
              </button>
            </div>
            <label class="toc-overlay-icon toc-content-icon" for="__toc">
              <div class="visually-hidden">Toggle table of contents sidebar</div>
              <i class="icon"><svg>
                  <use href="#svg-toc"></use>
                </svg></i>
            </label>
          </div>
          <article role="main">
            
            <div class="sphx-glr-example-title admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial is compatible with Gymnasium version 1.2.0.</p>
</div>
<section id="speeding-up-a2c-training-with-vector-envs">
<span id="sphx-glr-tutorials-training-agents-vector-a2c-py"></span><h1>Speeding up A2C Training with Vector Envs<a class="headerlink" href="#speeding-up-a2c-training-with-vector-envs" title="Link to this heading">¶</a></h1>
<p>This tutorial demonstrates training with vector environments to it speed up.</p>
<section id="notice">
<h2>Notice<a class="headerlink" href="#notice" title="Link to this heading">¶</a></h2>
<p>If you encounter an RuntimeError like the following comment raised on multiprocessing/spawn.py, wrap up the code from <code class="docutils literal notranslate"><span class="pre">gym.make_vec=</span></code> or <code class="docutils literal notranslate"><span class="pre">gym.vector.AsyncVectorEnv</span></code> to the end of the code by <code class="docutils literal notranslate"><span class="pre">if__name__</span> <span class="pre">==</span> <span class="pre">'__main__'</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">An</span> <span class="pre">attempt</span> <span class="pre">has</span> <span class="pre">been</span> <span class="pre">made</span> <span class="pre">to</span> <span class="pre">start</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">process</span> <span class="pre">before</span> <span class="pre">the</span> <span class="pre">current</span> <span class="pre">process</span> <span class="pre">has</span> <span class="pre">finished</span> <span class="pre">its</span> <span class="pre">bootstrapping</span> <span class="pre">phase.</span></code></p>
<hr class="docutils" />
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>In this tutorial, you’ll learn how to use vectorized environments to train an Advantage Actor-Critic agent.
We are going to use A2C, which is the synchronous version of the A3C algorithm [1].</p>
<p>Vectorized environments [3] can help to achieve quicker and more robust training by allowing multiple instances
of the same environment to run in parallel (on multiple CPUs). This can significantly reduce the variance and thus speeds up the training.</p>
<p>We will implement an Advantage Actor-Critic from scratch to look at how you can feed batched states into your networks to get a vector of actions
(one action per environment) and calculate the losses for actor and critic on minibatches of transitions.
Each minibatch contains the transitions of one sampling phase: <cite>n_steps_per_update</cite> steps are executed in <cite>n_envs</cite> environments in parallel
(multiply the two to get the number of transitions in a minibatch). After each sampling phase,  the losses are calculated and one gradient step is executed.
To calculate the advantages, we are going to use the Generalized Advantage Estimation (GAE) method [2], which balances the tradeoff
between variance and bias of the advantage estimates.</p>
<p>The A2C agent class is initialized with the number of features of the input state, the number of actions the agent can take,
the learning rates and the number of environments that run in parallel to collect experiences. The actor and critic networks are defined
and their respective optimizers are initialized. The forward pass of the networks takes in a batched vector of states and returns a tensor of state values
and a tensor of action logits. The select_action method returns a tuple of the chosen actions, the log-probs of those actions, and the state values for each action.
In addition, it also returns the entropy of the policy distribution, which is subtracted from the loss later (with a weighting factor <cite>ent_coef</cite>) to encourage exploration.</p>
<p>The get_losses function calculates the losses for the actor and critic networks (using GAE), which are then updated using the update_parameters function.</p>
<hr class="docutils" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Till Zemann</span>
<span class="c1"># License: MIT License</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>
</pre></div>
</div>
</section>
<section id="advantage-actor-critic-a2c">
<h2>Advantage Actor-Critic (A2C)<a class="headerlink" href="#advantage-actor-critic-a2c" title="Link to this heading">¶</a></h2>
<p>The Actor-Critic combines elements of value-based and policy-based methods. In A2C, the agent has two separate neural networks:
a critic network that estimates the state-value function, and an actor network that outputs logits for a categorical probability distribution over all actions.
The critic network is trained to minimize the mean squared error between the predicted state values and the actual returns received by the agent
(this is equivalent to minimizing the squared advantages, because the advantage of an action is as the difference between the return and the state-value: A(s,a) = Q(s,a) - V(s).
The actor network is trained to maximize the expected return by selecting actions that have high expected values according to the critic network.</p>
<p>The focus of this tutorial will not be on the details of A2C itself. Instead, the tutorial will focus on how to use vectorized environments
and domain randomization to accelerate the training process for A2C (and other reinforcement learning algorithms).</p>
<hr class="docutils" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">A2C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (Synchronous) Advantage Actor-Critic agent class</span>

<span class="sd">    Args:</span>
<span class="sd">        n_features: The number of features of the input state.</span>
<span class="sd">        n_actions: The number of actions the agent can take.</span>
<span class="sd">        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,</span>
<span class="sd">                for this code CPU is totally fine).</span>
<span class="sd">        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).</span>
<span class="sd">        actor_lr: The learning rate for the actor network.</span>
<span class="sd">        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="n">critic_lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">actor_lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">n_envs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the actor and critic networks and their respective optimizers.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_envs</span> <span class="o">=</span> <span class="n">n_envs</span>

        <span class="n">critic_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="c1"># estimate V(s)</span>
        <span class="p">]</span>

        <span class="n">actor_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
                <span class="mi">32</span><span class="p">,</span> <span class="n">n_actions</span>
            <span class="p">),</span>  <span class="c1"># estimate action logits (will be fed into a softmax later)</span>
        <span class="p">]</span>

        <span class="c1"># define actor and critic networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">critic_layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">actor_layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># define optimizers for actor and critic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_optim</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">critic_lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_optim</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">actor_lr</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of the networks.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: A batched vector of states.</span>

<span class="sd">        Returns:</span>
<span class="sd">            state_values: A tensor with the state values, with shape [n_envs,].</span>
<span class="sd">            action_logits_vec: A tensor with the action logits, with shape [n_envs, n_actions].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">state_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: [n_envs,]</span>
        <span class="n">action_logits_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: [n_envs, n_actions]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">state_values</span><span class="p">,</span> <span class="n">action_logits_vec</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a tuple of the chosen actions and the log-probs of those actions.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: A batched vector of states.</span>

<span class="sd">        Returns:</span>
<span class="sd">            actions: A tensor with the actions, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            action_log_probs: A tensor with the log-probs of the actions, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            state_values: A tensor with the state values, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_values</span><span class="p">,</span> <span class="n">action_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">action_pd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">action_logits</span>
        <span class="p">)</span>  <span class="c1"># implicitly uses softmax</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">action_pd</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">action_log_probs</span> <span class="o">=</span> <span class="n">action_pd</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">action_pd</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">action_log_probs</span><span class="p">,</span> <span class="n">state_values</span><span class="p">,</span> <span class="n">entropy</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_losses</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">action_log_probs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value_preds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">entropy</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">lam</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">ent_coef</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic</span>
<span class="sd">        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).</span>

<span class="sd">        Args:</span>
<span class="sd">            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].</span>
<span class="sd">            gamma: The discount factor.</span>
<span class="sd">            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,</span>
<span class="sd">                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased</span>
<span class="sd">                                          because the estimates are generated by a Neural Net).</span>
<span class="sd">            device: The device to run the computations on (e.g. CPU or GPU).</span>

<span class="sd">        Returns:</span>
<span class="sd">            critic_loss: The critic loss for the minibatch.</span>
<span class="sd">            actor_loss: The actor loss for the minibatch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># compute the advantages using GAE</span>
        <span class="n">gae</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="n">td_error</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">masks</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">value_preds</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">value_preds</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">gae</span> <span class="o">=</span> <span class="n">td_error</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">masks</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">gae</span>
            <span class="n">advantages</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">gae</span>

        <span class="c1"># calculate the loss of the minibatch for actor and critic</span>
        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">advantages</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># give a bonus for higher entropy to encourage exploration</span>
        <span class="n">actor_loss</span> <span class="o">=</span> <span class="p">(</span>
            <span class="o">-</span><span class="p">(</span><span class="n">advantages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">*</span> <span class="n">action_log_probs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">ent_coef</span> <span class="o">*</span> <span class="n">entropy</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">critic_loss</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_parameters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the parameters of the actor and critic networks.</span>

<span class="sd">        Args:</span>
<span class="sd">            critic_loss: The critic loss.</span>
<span class="sd">            actor_loss: The actor loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">critic_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic_optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">actor_optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">actor_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="using-vectorized-environments">
<h2>Using Vectorized Environments<a class="headerlink" href="#using-vectorized-environments" title="Link to this heading">¶</a></h2>
<p>When you calculate the losses for the two Neural Networks over only one epoch, it might have a high variance. With vectorized environments,
we can play with <cite>n_envs</cite> in parallel and thus get up to a linear speedup (meaning that in theory, we collect samples <cite>n_envs</cite> times quicker)
that we can use to calculate the loss for the current policy and critic network. When we are using more samples to calculate the loss,
it will have a lower variance and theirfore leads to quicker learning.</p>
<p>A2C is a synchronous method, meaning that the parameter updates to Networks take place deterministically (after each sampling phase),
but we can still make use of asynchronous vector envs to spawn multiple processes for parallel environment execution.</p>
<p>The simplest way to create vector environments is by calling <cite>gym.vector.make</cite>, which creates multiple instances of the same environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make_vec</span><span class="p">(</span><span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="domain-randomization">
<h2>Domain Randomization<a class="headerlink" href="#domain-randomization" title="Link to this heading">¶</a></h2>
<p>If we want to randomize the environment for training to get more robust agents (that can deal with different parameterizations of an environment
and theirfore might have a higher degree of generalization), we can set the desired parameters manually or use a pseudo-random number generator to generate them.</p>
<p>Manually setting up 3 parallel ‘LunarLander-v3’ envs with different parameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">SyncVectorEnv</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span>
            <span class="n">gravity</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span>
            <span class="n">enable_wind</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">wind_power</span><span class="o">=</span><span class="mf">15.0</span><span class="p">,</span>
            <span class="n">turbulence_power</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
            <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span>
            <span class="n">gravity</span><span class="o">=-</span><span class="mf">9.8</span><span class="p">,</span>
            <span class="n">enable_wind</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">wind_power</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
            <span class="n">turbulence_power</span><span class="o">=</span><span class="mf">1.3</span><span class="p">,</span>
            <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span> <span class="n">gravity</span><span class="o">=-</span><span class="mf">7.0</span><span class="p">,</span> <span class="n">enable_wind</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span>
        <span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Randomly generating the parameters for 3 parallel ‘LunarLander-v3’ envs, using <cite>np.clip</cite> to stay in the recommended parameter space:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">SyncVectorEnv</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span>
            <span class="n">gravity</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=-</span><span class="mf">11.99</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=-</span><span class="mf">0.01</span>
            <span class="p">),</span>
            <span class="n">enable_wind</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
            <span class="n">wind_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">15.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">19.99</span>
            <span class="p">),</span>
            <span class="n">turbulence_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">1.99</span>
            <span class="p">),</span>
            <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>Here we are using normal distributions with the standard parameterization of the environment as the mean and an arbitrary standard deviation (scale).
Depending on the problem, you can experiment with higher variance and use different distributions as well.</p>
<p>If you are training on the same <cite>n_envs</cite> environments for the entire training time, and <cite>n_envs</cite> is a relatively low number
(in proportion to how complex the environment is), you might still get some overfitting to the specific parameterizations that you picked.
To mitigate this, you can either pick a high number of randomly parameterized environments or remake your environments every couple of sampling phases
to generate a new set of pseudo-random parameters.</p>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># environment hyperparams</span>
<span class="n">n_envs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_updates</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_steps_per_update</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">randomize_domain</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># agent hyperparams</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.999</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mf">0.95</span>  <span class="c1"># hyperparameter for GAE</span>
<span class="n">ent_coef</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># coefficient for the entropy bonus (to encourage exploration)</span>
<span class="n">actor_lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">critic_lr</span> <span class="o">=</span> <span class="mf">0.005</span>

<span class="c1"># Note: the actor has a slower learning rate so that the value targets become</span>
<span class="c1"># more stationary and are theirfore easier to estimate for the critic</span>

<span class="c1"># environment setup</span>
<span class="k">if</span> <span class="n">randomize_domain</span><span class="p">:</span>
    <span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">AsyncVectorEnv</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
                <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span>
                <span class="n">gravity</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=-</span><span class="mf">11.99</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=-</span><span class="mf">0.01</span>
                <span class="p">),</span>
                <span class="n">enable_wind</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
                <span class="n">wind_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">15.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">19.99</span>
                <span class="p">),</span>
                <span class="n">turbulence_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">1.99</span>
                <span class="p">),</span>
                <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_envs</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="n">envs</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make_vec</span><span class="p">(</span><span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span> <span class="n">num_envs</span><span class="o">=</span><span class="n">n_envs</span><span class="p">,</span> <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>


<span class="n">obs_shape</span> <span class="o">=</span> <span class="n">envs</span><span class="o">.</span><span class="n">single_observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">action_shape</span> <span class="o">=</span> <span class="n">envs</span><span class="o">.</span><span class="n">single_action_space</span><span class="o">.</span><span class="n">n</span>

<span class="c1"># set the device</span>
<span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># init the agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">action_shape</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">critic_lr</span><span class="p">,</span> <span class="n">actor_lr</span><span class="p">,</span> <span class="n">n_envs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-a2c-agent">
<h2>Training the A2C Agent<a class="headerlink" href="#training-the-a2c-agent" title="Link to this heading">¶</a></h2>
<p>For our training loop, we are using the <cite>RecordEpisodeStatistics</cite> wrapper to record the episode lengths and returns and we are also saving
the losses and entropies to plot them after the agent finished training.</p>
<p>You may notice that we don’t reset the vectorized envs at the start of each episode like we would usually do.
This is because each environment resets automatically once the episode finishes (each environment takes a different number of timesteps to finish
an episode because of the random seeds). As a result, we are also not collecting data in <cite>episodes</cite>, but rather just play a certain number of steps
(<cite>n_steps_per_update</cite>) in each environment (as an example, this could mean that we play 20 timesteps to finish an episode and then
use the rest of the timesteps to begin a new one).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a wrapper environment to save episode returns and episode lengths</span>
<span class="n">envs_wrapper</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">RecordEpisodeStatistics</span><span class="p">(</span>
    <span class="n">envs</span><span class="p">,</span> <span class="n">buffer_length</span><span class="o">=</span><span class="n">n_envs</span> <span class="o">*</span> <span class="n">n_updates</span>
<span class="p">)</span>

<span class="n">critic_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">actor_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">entropies</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># use tqdm to get a progress bar for training</span>
<span class="k">for</span> <span class="n">sample_phase</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_updates</span><span class="p">)):</span>
    <span class="c1"># we don&#39;t have to reset the envs, they just continue playing</span>
    <span class="c1"># until the episode is over and then reset automatically</span>

    <span class="c1"># reset lists that collect experiences of an episode (sample phase)</span>
    <span class="n">ep_value_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_steps_per_update</span><span class="p">,</span> <span class="n">n_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ep_rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_steps_per_update</span><span class="p">,</span> <span class="n">n_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">ep_action_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_steps_per_update</span><span class="p">,</span> <span class="n">n_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_steps_per_update</span><span class="p">,</span> <span class="n">n_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># at the start of training reset all envs to get an initial state</span>
    <span class="k">if</span> <span class="n">sample_phase</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">envs_wrapper</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># play n steps in our parallel environments to collect data</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps_per_update</span><span class="p">):</span>
        <span class="c1"># select an action A_{t} using S_{t} as input for the agent</span>
        <span class="n">actions</span><span class="p">,</span> <span class="n">action_log_probs</span><span class="p">,</span> <span class="n">state_value_preds</span><span class="p">,</span> <span class="n">entropy</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span>
            <span class="n">states</span>
        <span class="p">)</span>

        <span class="c1"># perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">envs_wrapper</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">ep_value_preds</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">state_value_preds</span><span class="p">)</span>
        <span class="n">ep_rewards</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ep_action_log_probs</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">action_log_probs</span>

        <span class="c1"># add a mask (for the return calculation later);</span>
        <span class="c1"># for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)</span>
        <span class="n">masks</span><span class="p">[</span><span class="n">step</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="ow">not</span> <span class="n">term</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">terminated</span><span class="p">])</span>

    <span class="c1"># calculate the losses for actor and critic</span>
    <span class="n">critic_loss</span><span class="p">,</span> <span class="n">actor_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">get_losses</span><span class="p">(</span>
        <span class="n">ep_rewards</span><span class="p">,</span>
        <span class="n">ep_action_log_probs</span><span class="p">,</span>
        <span class="n">ep_value_preds</span><span class="p">,</span>
        <span class="n">entropy</span><span class="p">,</span>
        <span class="n">masks</span><span class="p">,</span>
        <span class="n">gamma</span><span class="p">,</span>
        <span class="n">lam</span><span class="p">,</span>
        <span class="n">ent_coef</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># update the actor and critic networks</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">)</span>

    <span class="c1"># log the losses and entropy</span>
    <span class="n">critic_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">actor_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="plotting">
<h2>Plotting<a class="headerlink" href="#plotting" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; plot the results &quot;&quot;&quot;</span>

<span class="c1"># %matplotlib inline</span>

<span class="n">rolling_length</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Training plots for </span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> in the LunarLander-v3 environment </span><span class="se">\n</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">             (n_envs=</span><span class="si">{</span><span class="n">n_envs</span><span class="si">}</span><span class="s2">, n_steps_per_update=</span><span class="si">{</span><span class="n">n_steps_per_update</span><span class="si">}</span><span class="s2">, randomize_domain=</span><span class="si">{</span><span class="n">randomize_domain</span><span class="si">}</span><span class="s2">)&quot;</span>
<span class="p">)</span>

<span class="c1"># episode return</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Episode Returns&quot;</span><span class="p">)</span>
<span class="n">episode_returns_moving_average</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">envs_wrapper</span><span class="o">.</span><span class="n">return_queue</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">rolling_length</span><span class="p">),</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="o">/</span> <span class="n">rolling_length</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">episode_returns_moving_average</span><span class="p">))</span> <span class="o">/</span> <span class="n">n_envs</span><span class="p">,</span>
    <span class="n">episode_returns_moving_average</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of episodes&quot;</span><span class="p">)</span>

<span class="c1"># entropy</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Entropy&quot;</span><span class="p">)</span>
<span class="n">entropy_moving_average</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">entropies</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">rolling_length</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
    <span class="o">/</span> <span class="n">rolling_length</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">entropy_moving_average</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of updates&quot;</span><span class="p">)</span>


<span class="c1"># critic loss</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Critic Loss&quot;</span><span class="p">)</span>
<span class="n">critic_losses_moving_average</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">rolling_length</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span>
    <span class="p">)</span>
    <span class="o">/</span> <span class="n">rolling_length</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">critic_losses_moving_average</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of updates&quot;</span><span class="p">)</span>


<span class="c1"># actor loss</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Actor Loss&quot;</span><span class="p">)</span>
<span class="n">actor_losses_moving_average</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">rolling_length</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
    <span class="o">/</span> <span class="n">rolling_length</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">actor_losses_moving_average</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of updates&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="training_plots" src="../../../_images/vector_env_a2c_training_plots.png" />
</section>
<section id="performance-analysis-of-synchronous-and-asynchronous-vectorized-environments">
<h2>Performance Analysis of Synchronous and Asynchronous Vectorized Environments<a class="headerlink" href="#performance-analysis-of-synchronous-and-asynchronous-vectorized-environments" title="Link to this heading">¶</a></h2>
<hr class="docutils" />
<p>Asynchronous environments can lead to quicker training times and a higher speedup
for data collection compared to synchronous environments. This is because asynchronous environments
allow multiple agents to interact with their environments in parallel,
while synchronous environments run multiple environments serially.
This results in better efficiency and faster training times for asynchronous environments.</p>
<img alt="performance_plots" src="../../../_images/vector_env_performance_plots.png" />
<hr class="docutils" />
<p>According to the Karp-Flatt metric (a metric used in parallel computing to estimate the limit for the
speedup when scaling up the number of parallel processes, here the number of environments),
the estimated max. speedup for asynchronous environments is 57, while the estimated maximum speedup
for synchronous environments is 21. This suggests that asynchronous environments have significantly
faster training times compared to synchronous environments (see graphs).</p>
<img alt="karp_flatt_metric" src="../../../_images/vector_env_karp_flatt_plot.png" />
<hr class="docutils" />
<p>However, it is important to note that increasing the number of parallel vector environments
can lead to slower training times after a certain number of environments (see plot below, where the
agent was trained until the mean training returns were above -120). The slower training times might occur
because the gradients of the environments are good enough after a relatively low number of environments
(especially if the environment is not very complex). In this case, increasing the number of environments
does not increase the learning speed, and actually increases the runtime, possibly due to the additional time
needed to calculate the gradients. For LunarLander-v3, the best performing configuration used a AsyncVectorEnv
with 10 parallel environments, but environments with a higher complexity may require more
parallel environments to achieve optimal performance.</p>
<img alt="runtime_until_threshold_plot" src="../../../_images/vector_env_runtime_until_threshold.png" />
</section>
<section id="saving-loading-weights">
<h2>Saving/ Loading Weights<a class="headerlink" href="#saving-loading-weights" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">load_weights</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">actor_weights_path</span> <span class="o">=</span> <span class="s2">&quot;weights/actor_weights.h5&quot;</span>
<span class="n">critic_weights_path</span> <span class="o">=</span> <span class="s2">&quot;weights/critic_weights.h5&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot; save network weights &quot;&quot;&quot;</span>
<span class="k">if</span> <span class="n">save_weights</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">actor_weights_path</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">critic_weights_path</span><span class="p">)</span>


<span class="sd">&quot;&quot;&quot; load network weights &quot;&quot;&quot;</span>
<span class="k">if</span> <span class="n">load_weights</span><span class="p">:</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">action_shape</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">critic_lr</span><span class="p">,</span> <span class="n">actor_lr</span><span class="p">)</span>

    <span class="n">agent</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">actor_weights_path</span><span class="p">))</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">critic_weights_path</span><span class="p">))</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="showcase-the-agent">
<h2>Showcase the Agent<a class="headerlink" href="#showcase-the-agent" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; play a couple of showcase episodes &quot;&quot;&quot;</span>

<span class="n">n_showcase_episodes</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_showcase_episodes</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;starting episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

    <span class="c1"># create a new sample environment to get new random parameters</span>
    <span class="k">if</span> <span class="n">randomize_domain</span><span class="p">:</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
            <span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span>
            <span class="n">render_mode</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">,</span>
            <span class="n">gravity</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=-</span><span class="mf">11.99</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=-</span><span class="mf">0.01</span>
            <span class="p">),</span>
            <span class="n">enable_wind</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
            <span class="n">wind_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">15.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">19.99</span>
            <span class="p">),</span>
            <span class="n">turbulence_power</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">a_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a_max</span><span class="o">=</span><span class="mf">1.99</span>
            <span class="p">),</span>
            <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v3&quot;</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="c1"># get an initial state</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># play one episode</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># select an action A_{t} using S_{t} as input for the agent</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span>

        <span class="c1"># perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># update if the environment is done</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="try-playing-the-environment-yourself">
<h2>Try playing the environment yourself<a class="headerlink" href="#try-playing-the-environment-yourself" title="Link to this heading">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># from gymnasium.utils.play import play</span>
<span class="c1">#</span>
<span class="c1"># play(gym.make(&#39;LunarLander-v3&#39;, render_mode=&#39;rgb_array&#39;),</span>
<span class="c1">#     keys_to_action={&#39;w&#39;: 2, &#39;a&#39;: 1, &#39;d&#39;: 3}, noop=0)</span>
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<p>[1] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu. “Asynchronous Methods for Deep Reinforcement Learning” ICML (2016).</p>
<p>[2] J. Schulman, P. Moritz, S. Levine, M. Jordan and P. Abbeel. “High-dimensional continuous control using generalized advantage estimation.” ICLR (2016).</p>
<p>[3] Gymnasium Documentation: Vector environments. (URL: <a class="reference external" href="https://gymnasium.farama.org/api/vector/">https://gymnasium.farama.org/api/vector/</a>)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-training-agents-vector-a2c-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/c45e443052d6f580d9746d1bb3a70ebc/vector_a2c.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">vector_a2c.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/fedb802640e3ee4762d570997604c039/vector_a2c.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">vector_a2c.ipynb</span></code></a></p>
</div>
</div>
</section>
</section>

          </article>
        </div>
        <footer>
          
          <div class="related-pages">
            <a class="next-page" href="../../third-party-tutorials/">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Third-Party Tutorials</div>
              </div>
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
            </a>
            <a class="prev-page" href="../mujoco_reinforce/">
              <svg class="furo-related-icon">
                <use href="#svg-arrow-right"></use>
              </svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Training using REINFORCE for Mujoco</div>
                
              </div>
            </a>
          </div>
          <div class="bottom-of-page">
            <div class="left-details">
              <div class="copyright">
                Copyright &#169; 2025 Farama Foundation
              </div>
              <!--
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            -->
            </div>
            <div class="right-details">
              <div class="icons">
                <a class="muted-link" href="https://github.com/Farama-Foundation/Gymnasium/"
                  aria-label="On GitHub">
                  <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd"
                      d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z">
                    </path>
                  </svg>
                </a>
              </div>
            </div>
          </div>
          
        </footer>
      </div>
      <aside class="toc-drawer">
        
        
        <div class="toc-sticky toc-scroll">
          <div class="toc-title-container">
            <span class="toc-title">
              On this page
            </span>
          </div>
          <div class="toc-tree-container">
            <div class="toc-tree">
              <ul>
<li><a class="reference internal" href="#">Speeding up A2C Training with Vector Envs</a><ul>
<li><a class="reference internal" href="#notice">Notice</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</a></li>
<li><a class="reference internal" href="#using-vectorized-environments">Using Vectorized Environments</a></li>
<li><a class="reference internal" href="#domain-randomization">Domain Randomization</a></li>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#training-the-a2c-agent">Training the A2C Agent</a></li>
<li><a class="reference internal" href="#plotting">Plotting</a></li>
<li><a class="reference internal" href="#performance-analysis-of-synchronous-and-asynchronous-vectorized-environments">Performance Analysis of Synchronous and Asynchronous Vectorized Environments</a></li>
<li><a class="reference internal" href="#saving-loading-weights">Saving/ Loading Weights</a></li>
<li><a class="reference internal" href="#showcase-the-agent">Showcase the Agent</a></li>
<li><a class="reference internal" href="#try-playing-the-environment-yourself">Try playing the environment yourself</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
        
        
      </aside>
    </div>
  </div>
</div>
    <script>
      const toggleMenu = () => {
        const menuBtn = document.querySelector(".farama-header-menu__btn");
        const menuContainer = document.querySelector(".farama-header-menu-container");
        if (document.querySelector(".farama-header-menu").classList.contains("active")) {
          menuBtn.setAttribute("aria-expanded", "false");
          menuContainer.setAttribute("aria-hidden", "true");
        } else {
          menuBtn.setAttribute("aria-expanded", "true");
          menuContainer.setAttribute("aria-hidden", "false");
        }
        document.querySelector(".farama-header-menu").classList.toggle("active");
      }

      document.querySelector(".farama-header-menu__btn").addEventListener("click", toggleMenu);
      document.getElementById("farama-close-menu").addEventListener("click", toggleMenu);
    </script>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6H9C8TWXZ8"></script>
      <script>
        const enableGtag = () => {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-6H9C8TWXZ8');
        }
        (() => {
            if (!localStorage.getItem("acceptedCookieAlert")) {
                const boxElem = document.createElement("div");
                boxElem.classList.add("cookie-alert");
                const containerElem = document.createElement("div");
                containerElem.classList.add("cookie-alert__container");
                const textElem = document.createElement("p");
                textElem.innerHTML = `This page uses <a href="https://analytics.google.com/">
                                    Google Analytics</a> to collect statistics.`;
                                    containerElem.appendChild(textElem);

                const declineBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Deny",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__decline",
                  }
                );
                declineBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", false);
                  boxElem.remove();
                });

                const acceptBtn = Object.assign(document.createElement("button"),
                  {
                    innerText: "Allow",
                    className: "farama-btn cookie-alert__button",
                    id: "cookie-alert__accept",
                  }
                );
                acceptBtn.addEventListener("click", () => {
                  localStorage.setItem("acceptedCookieAlert", true);
                  boxElem.remove();
                  enableGtag();
                });

                containerElem.appendChild(declineBtn);
                containerElem.appendChild(acceptBtn);
                boxElem.appendChild(containerElem);
                document.body.appendChild(boxElem);
            } else if (localStorage.getItem("acceptedCookieAlert") === "true") {
              enableGtag();
            }
        })()
      </script>

    <script src="../../../_static/documentation_options.js?v=151cd43d"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/furo.js?v=7660844c"></script>
    
    <script>

      const createProjectsList = (projects, displayImages) => {
        const ulElem = Object.assign(document.createElement('ul'),
          {
            className:'farama-header-menu-list',
          }
        )
        for (let project of projects) {
          const liElem = document.createElement("li");
          const aElem = Object.assign(document.createElement("a"),
            {
              href: project.link
            }
          );
          liElem.appendChild(aElem);
          if (displayImages) {
            const imgElem = Object.assign(document.createElement("img"),
              {
                src: project.image ? imagesBasepath + project.image : imagesBasepath + "/farama_black.svg",
                alt: `${project.name} logo`,
                className: "farama-black-logo-invert"
              }
            );
            aElem.appendChild(imgElem);
          }
          aElem.appendChild(document.createTextNode(project.name));
          ulElem.appendChild(liElem);
        }
        return ulElem;
      }

      // Create menu with Farama projects by using the API at farama.org/api/projects.json
      const createCORSRequest = (method, url) => {
        let xhr = new XMLHttpRequest();
        xhr.responseType = 'json';

        if ("withCredentials" in xhr) {
          xhr.open(method, url, true);
        } else if (typeof XDomainRequest != "undefined") {
          // IE8 & IE9
          xhr = new XDomainRequest();
          xhr.open(method, url);
        } else {
          // CORS not supported.
          xhr = null;
        }
        return xhr;
      };

      const url = 'https://farama.org/api/projects.json';
      const imagesBasepath = "https://farama.org/assets/images"
      const method = 'GET';
      let xhr = createCORSRequest(method, url);

      xhr.onload = () => {
        const jsonResponse = xhr.response;
        const sections = {
          "Core Projects": [],
          "Mature Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Incubating Projects": {
            "Documentation": [],
            "Repositories": [],
          },
          "Foundation": [
            {
              name: "About",
              link: "https://farama.org/about"
            },
            {
              name: "Standards",
              link: "https://farama.org/project_standards",
            },
            {
              name: "Donate",
              link: "https://farama.org/donations"
            }
          ]
        }

        // Categorize projects
        Object.keys(jsonResponse).forEach(key => {
          projectJson = jsonResponse[key];
          if (projectJson.website !== null) {
            projectJson.link = projectJson.website;
          } else {
            projectJson.link = projectJson.github;
          }
          if (projectJson.type === "core") {
            sections["Core Projects"].push(projectJson)
          } else if (projectJson.type == "mature") {
            if (projectJson.website !== null) {
              sections["Mature Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Mature Projects"]["Repositories"].push(projectJson)
            }
          } else {
            if (projectJson.website !== null) {
              sections["Incubating Projects"]["Documentation"].push(projectJson)
            } else {
              sections["Incubating Projects"]["Repositories"].push(projectJson)
            }
          }
        })

        const menuContainer = document.querySelector(".farama-header-menu__body");

        Object.keys(sections).forEach((key, i) => {
          const sectionElem = Object.assign(
            document.createElement('div'), {
              className:'farama-header-menu__section',
            }
          )
          sectionElem.appendChild(Object.assign(document.createElement('span'),
            {
              className:'farama-header-menu__section-title' ,
              innerText: key
            }
          ))
          // is not a list
          if (sections[key].constructor !== Array) {
            const subSections = sections[key];
            const subSectionContainerElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsections-container',
                  style: 'display: flex'
                }
            )
            Object.keys(subSections).forEach((subKey, i) => {
              const subSectionElem = Object.assign(
                document.createElement('div'), {
                  className:'farama-header-menu__subsection',
                }
              )
              subSectionElem.appendChild(Object.assign(document.createElement('span'),
                {
                  className:'farama-header-menu__subsection-title' ,
                  innerText: subKey
                }
              ))
              const ulElem = createProjectsList(subSections[subKey], key !== 'Foundation');
              subSectionElem.appendChild(ulElem);
              subSectionContainerElem.appendChild(subSectionElem);
            })
            sectionElem.appendChild(subSectionContainerElem);
          } else {
            const projects = sections[key];
            const ulElem = createProjectsList(projects, true);
            sectionElem.appendChild(ulElem);
          }
          menuContainer.appendChild(sectionElem)
        });
      }

      xhr.onerror = function() {
        console.error("Unable to load projects");
      };

      xhr.send();
    </script>

    
    <script>
      const versioningConfig = {
        githubUser: 'Farama-Foundation',
        githubRepo: 'Gymnasium',
      };
      fetch('/main/_static/versioning/versioning_menu.html').then(response => {
        if (response.status === 200) {
            response.text().then(text => {
                const container = document.createElement("div");
                container.innerHTML = text;
                document.querySelector("body").appendChild(container);
                // innerHtml doenst evaluate scripts, we need to add them dynamically
                Array.from(container.querySelectorAll("script")).forEach(oldScript => {
                    const newScript = document.createElement("script");
                    Array.from(oldScript.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value));
                    newScript.appendChild(document.createTextNode(oldScript.innerHTML));
                    oldScript.parentNode.replaceChild(newScript, oldScript);
                });
            });
        } else {
            console.warn("Unable to load versioning menu", response);
        }
      });
    </script>

    </body>
</html>