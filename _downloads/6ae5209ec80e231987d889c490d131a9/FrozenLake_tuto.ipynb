{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Frozenlake benchmark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this post we'll compare a bunch of different map sizes on the\n[FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)_\nenvironment from the reinforcement learning\n[Gymnasium](https://gymnasium.farama.org/)_ package using the\nQ-learning algorithm.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first import a few dependencies we'll need.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Andrea Pierr\u00e9\n# License: MIT License\n\n\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport gymnasium as gym\nfrom gymnasium.envs.toy_text.frozen_lake import generate_random_map\n\n\nsns.set_theme()\n\n# %load_ext lab_black"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters we'll use\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Params(NamedTuple):\n    total_episodes: int  # Total episodes\n    learning_rate: float  # Learning rate\n    gamma: float  # Discounting rate\n    epsilon: float  # Exploration probability\n    map_size: int  # Number of tiles of one side of the squared environment\n    seed: int  # Define a seed so that we get reproducible results\n    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n    n_runs: int  # Number of runs\n    action_size: int  # Number of possible actions\n    state_size: int  # Number of possible states\n    proba_frozen: float  # Probability that a tile is frozen\n    savefig_folder: Path  # Root folder where plots are saved\n\n\nparams = Params(\n    total_episodes=2000,\n    learning_rate=0.8,\n    gamma=0.95,\n    epsilon=0.1,\n    map_size=5,\n    seed=123,\n    is_slippery=False,\n    n_runs=20,\n    action_size=None,\n    state_size=None,\n    proba_frozen=0.9,\n    savefig_folder=Path(\"../../_static/img/tutorials/\"),\n)\nparams\n\n# Set the seed\nrng = np.random.default_rng(params.seed)\n\n# Create the figure folder if it doesn't exist\nparams.savefig_folder.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The FrozenLake environment\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n    \"FrozenLake-v1\",\n    is_slippery=params.is_slippery,\n    render_mode=\"rgb_array\",\n    desc=generate_random_map(\n        size=params.map_size, p=params.proba_frozen, seed=params.seed\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the Q-table\n\nIn this tutorial we'll be using Q-learning as our learning algorithm and\n$\\epsilon$-greedy to decide which action to pick at each step. You\ncan have a look at the [References section](#References)_ for some\nrefreshers on the theory. Now, let's create our Q-table initialized at\nzero with the states number as rows and the actions number as columns.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = params._replace(action_size=env.action_space.n)\nparams = params._replace(state_size=env.observation_space.n)\nprint(f\"Action size: {params.action_size}\")\nprint(f\"State size: {params.state_size}\")\n\n\nclass Qlearning:\n    def __init__(self, learning_rate, gamma, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.reset_qtable()\n\n    def update(self, state, action, reward, new_state):\n        \"\"\"Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\"\"\"\n        delta = (\n            reward\n            + self.gamma * np.max(self.qtable[new_state, :])\n            - self.qtable[state, action]\n        )\n        q_update = self.qtable[state, action] + self.learning_rate * delta\n        return q_update\n\n    def reset_qtable(self):\n        \"\"\"Reset the Q-table.\"\"\"\n        self.qtable = np.zeros((self.state_size, self.action_size))\n\n\nclass EpsilonGreedy:\n    def __init__(self, epsilon):\n        self.epsilon = epsilon\n\n    def choose_action(self, action_space, state, qtable):\n        \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n        # First we randomize a number\n        explor_exploit_tradeoff = rng.uniform(0, 1)\n\n        # Exploration\n        if explor_exploit_tradeoff < self.epsilon:\n            action = action_space.sample()\n\n        # Exploitation (taking the biggest Q-value for this state)\n        else:\n            # Break ties randomly\n            # Find the indices where the Q-value equals the maximum value\n            # Choose a random action from the indices where the Q-value is maximum\n            max_ids = np.where(qtable[state, :] == max(qtable[state, :]))[0]\n            action = rng.choice(max_ids)\n        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running the environment\n\nLet's instantiate the learner and the explorer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner = Qlearning(\n    learning_rate=params.learning_rate,\n    gamma=params.gamma,\n    state_size=params.state_size,\n    action_size=params.action_size,\n)\nexplorer = EpsilonGreedy(\n    epsilon=params.epsilon,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will be our main function to run our environment until the maximum\nnumber of episodes ``params.total_episodes``. To account for\nstochasticity, we will also run our environment a few times.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_env():\n    rewards = np.zeros((params.total_episodes, params.n_runs))\n    steps = np.zeros((params.total_episodes, params.n_runs))\n    episodes = np.arange(params.total_episodes)\n    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n    all_states = []\n    all_actions = []\n\n    for run in range(params.n_runs):  # Run several times to account for stochasticity\n        learner.reset_qtable()  # Reset the Q-table between runs\n\n        for episode in tqdm(\n            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n        ):\n            state = env.reset(seed=params.seed)[0]  # Reset the environment\n            step = 0\n            done = False\n            total_rewards = 0\n\n            while not done:\n                action = explorer.choose_action(\n                    action_space=env.action_space, state=state, qtable=learner.qtable\n                )\n\n                # Log all states and actions\n                all_states.append(state)\n                all_actions.append(action)\n\n                # Take the action (a) and observe the outcome state(s') and reward (r)\n                new_state, reward, terminated, truncated, info = env.step(action)\n\n                done = terminated or truncated\n\n                learner.qtable[state, action] = learner.update(\n                    state, action, reward, new_state\n                )\n\n                total_rewards += reward\n                step += 1\n\n                # Our new state is state\n                state = new_state\n\n            # Log all rewards and steps\n            rewards[episode, run] = total_rewards\n            steps[episode, run] = step\n        qtables[run, :, :] = learner.qtable\n\n    return rewards, steps, episodes, qtables, all_states, all_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make it easy to plot the results with Seaborn, we'll save the main\nresults of the simulation in Pandas dataframes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def postprocess(episodes, params, rewards, steps, map_size):\n    \"\"\"Convert the results of the simulation in dataframes.\"\"\"\n    res = pd.DataFrame(\n        data={\n            \"Episodes\": np.tile(episodes, reps=params.n_runs),\n            \"Rewards\": rewards.flatten(order=\"F\"),\n            \"Steps\": steps.flatten(order=\"F\"),\n        }\n    )\n    res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n    res[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", res.shape[0])\n\n    st = pd.DataFrame(data={\"Episodes\": episodes, \"Steps\": steps.mean(axis=1)})\n    st[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", st.shape[0])\n    return res, st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to plot the policy the agent has learned in the end. To do that\nwe will: 1. extract the best Q-values from the Q-table for each state,\n2. get the corresponding best action for those Q-values, 3. map each\naction to an arrow so we can visualize it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def qtable_directions_map(qtable, map_size):\n    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\n    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\n    directions = {0: \"\u2190\", 1: \"\u2193\", 2: \"\u2192\", 3: \"\u2191\"}\n    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n    eps = np.finfo(float).eps  # Minimum float number on the machine\n    for idx, val in enumerate(qtable_best_action.flatten()):\n        if qtable_val_max.flatten()[idx] > eps:\n            # Assign an arrow only if a minimal Q-value has been learned as best action\n            # otherwise since 0 is a direction, it also gets mapped on the tiles where\n            # it didn't actually learn anything\n            qtable_directions[idx] = directions[val]\n    qtable_directions = qtable_directions.reshape(map_size, map_size)\n    return qtable_val_max, qtable_directions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the following function, we'll plot on the left the last frame of\nthe simulation. If the agent learned a good policy to solve the task, we\nexpect to see it on the tile of the treasure in the last frame of the\nvideo. On the right we'll plot the policy the agent has learned. Each\narrow will represent the best action to choose for each tile/state.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_q_values_map(qtable, env, map_size):\n    \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n    qtable_val_max, qtable_directions = qtable_directions_map(qtable, map_size)\n\n    # Plot the last frame\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    ax[0].imshow(env.render())\n    ax[0].axis(\"off\")\n    ax[0].set_title(\"Last frame\")\n\n    # Plot the policy\n    sns.heatmap(\n        qtable_val_max,\n        annot=qtable_directions,\n        fmt=\"\",\n        ax=ax[1],\n        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n        linewidths=0.7,\n        linecolor=\"black\",\n        xticklabels=[],\n        yticklabels=[],\n        annot_kws={\"fontsize\": \"xx-large\"},\n    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n    for _, spine in ax[1].spines.items():\n        spine.set_visible(True)\n        spine.set_linewidth(0.7)\n        spine.set_color(\"black\")\n    img_title = f\"frozenlake_q_values_{map_size}x{map_size}.png\"\n    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a sanity check, we will plot the distributions of states and actions\nwith the following function:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_states_actions_distribution(states, actions, map_size):\n    \"\"\"Plot the distributions of states and actions.\"\"\"\n    labels = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    sns.histplot(data=states, ax=ax[0], kde=True)\n    ax[0].set_title(\"States\")\n    sns.histplot(data=actions, ax=ax[1])\n    ax[1].set_xticks(list(labels.values()), labels=labels.keys())\n    ax[1].set_title(\"Actions\")\n    fig.tight_layout()\n    img_title = f\"frozenlake_states_actions_distrib_{map_size}x{map_size}.png\"\n    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we'll be running our agent on a few increasing maps sizes: -\n$4 \\times 4$, - $7 \\times 7$, - $9 \\times 9$, -\n$11 \\times 11$.\n\nPutting it all together:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "map_sizes = [4, 7, 9, 11]\nres_all = pd.DataFrame()\nst_all = pd.DataFrame()\n\nfor map_size in map_sizes:\n    env = gym.make(\n        \"FrozenLake-v1\",\n        is_slippery=params.is_slippery,\n        render_mode=\"rgb_array\",\n        desc=generate_random_map(\n            size=map_size, p=params.proba_frozen, seed=params.seed\n        ),\n    )\n\n    params = params._replace(action_size=env.action_space.n)\n    params = params._replace(state_size=env.observation_space.n)\n    env.action_space.seed(\n        params.seed\n    )  # Set the seed to get reproducible results when sampling the action space\n    learner = Qlearning(\n        learning_rate=params.learning_rate,\n        gamma=params.gamma,\n        state_size=params.state_size,\n        action_size=params.action_size,\n    )\n    explorer = EpsilonGreedy(\n        epsilon=params.epsilon,\n    )\n\n    print(f\"Map size: {map_size}x{map_size}\")\n    rewards, steps, episodes, qtables, all_states, all_actions = run_env()\n\n    # Save the results in dataframes\n    res, st = postprocess(episodes, params, rewards, steps, map_size)\n    res_all = pd.concat([res_all, res])\n    st_all = pd.concat([st_all, st])\n    qtable = qtables.mean(axis=0)  # Average the Q-table between runs\n\n    plot_states_actions_distribution(\n        states=all_states, actions=all_actions, map_size=map_size\n    )  # Sanity check\n    plot_q_values_map(qtable, env, map_size)\n\n    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Map size: $4 \\times 4$\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|States actions histogram 4x4 map| |Q-values 4x4 map|\n\n.. |States actions histogram 4x4 map| image:: ../../_static/img/tutorials/frozenlake_states_actions_distrib_4x4.png\n.. |Q-values 4x4 map| image:: ../../_static/img/tutorials/frozenlake_q_values_4x4.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Map size: $7 \\times 7$\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|States actions histogram 7x7 map| |Q-values 7x7 map|\n\n.. |States actions histogram 7x7 map| image:: ../../_static/img/tutorials/frozenlake_states_actions_distrib_7x7.png\n.. |Q-values 7x7 map| image:: ../../_static/img/tutorials/frozenlake_q_values_7x7.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Map size: $9 \\times 9$\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|States actions histogram 9x9 map| |Q-values 9x9 map|\n\n.. |States actions histogram 9x9 map| image:: ../../_static/img/tutorials/frozenlake_states_actions_distrib_9x9.png\n.. |Q-values 9x9 map| image:: ../../_static/img/tutorials/frozenlake_q_values_9x9.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Map size: $11 \\times 11$\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|States actions histogram 11x11 map| |Q-values 11x11 map|\n\n.. |States actions histogram 11x11 map| image:: ../../_static/img/tutorials/frozenlake_states_actions_distrib_11x11.png\n.. |Q-values 11x11 map| image:: ../../_static/img/tutorials/frozenlake_q_values_11x11.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``DOWN`` and ``RIGHT`` actions get chosen more often, which makes\nsense as the agent starts at the top left of the map and needs to find\nits way down to the bottom right. Also the bigger the map, the less\nstates/tiles further away from the starting state get visited.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check if our agent is learning, we want to plot the cumulated sum of\nrewards, as well as the number of steps needed until the end of the\nepisode. If our agent is learning, we expect to see the cumulated sum of\nrewards to increase and the number of steps to solve the task to\ndecrease.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_steps_and_rewards(rewards_df, steps_df):\n    \"\"\"Plot the steps and rewards from dataframes.\"\"\"\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    sns.lineplot(\n        data=rewards_df, x=\"Episodes\", y=\"cum_rewards\", hue=\"map_size\", ax=ax[0]\n    )\n    ax[0].set(ylabel=\"Cumulated rewards\")\n\n    sns.lineplot(data=steps_df, x=\"Episodes\", y=\"Steps\", hue=\"map_size\", ax=ax[1])\n    ax[1].set(ylabel=\"Averaged steps number\")\n\n    for axi in ax:\n        axi.legend(title=\"map size\")\n    fig.tight_layout()\n    img_title = \"frozenlake_steps_and_rewards.png\"\n    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n    plt.show()\n\n\nplot_steps_and_rewards(res_all, st_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|Steps and rewards|\n\n.. |Steps and rewards| image:: ../../_static/img/tutorials/frozenlake_steps_and_rewards.png\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the $4 \\times 4$ map, learning converges pretty quickly,\nwhereas on the $7 \\times 7$ map, the agent needs $\\sim 300$\nepisodes, on the $9 \\times 9$ map it needs $\\sim 800$\nepisodes, and the $11 \\times 11$ map, it needs $\\sim 1800$\nepisodes to converge. Interestingly, the agent seems to be getting more\nrewards on the $9 \\times 9$ map than on the $7 \\times 7$\nmap, which could mean it didn't reach an optimal policy on the\n$7 \\times 7$ map.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end, if agent doesn't get any rewards, rewards don't get\npropagated in the Q-values, and the agent doesn't learn anything. In my\nexperience on this environment using $\\epsilon$-greedy and those\nhyperparameters and environment settings, maps having more than\n$11 \\times 11$ tiles start to be difficult to solve. Maybe using a\ndifferent exploration algorithm could overcome this. The other parameter\nhaving a big impact is the ``proba_frozen``, the probability of the tile\nbeing frozen. With too many holes, i.e. $p<0.9$, Q-learning is\nhaving a hard time in not falling into holes and getting a reward\nsignal.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n-  Code inspired by [Deep Reinforcement Learning\n   Course](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)_\n   by Thomas Simonini (http://simoninithomas.com/)\n-  [Dissecting Reinforcement\n   Learning-Part.2](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)_\n-  [David Silver\u2019s course](https://www.davidsilver.uk/teaching/)_ in\n   particular lesson 4 and lesson 5\n-  [Q-learning article on\n   Wikipedia](https://en.wikipedia.org/wiki/Q-learning)_\n-  [Q-Learning: Off-Policy TD\n   Control](http://incompleteideas.net/book/ebook/node65.html)_ in\n   [Reinforcement Learning: An Introduction, by Richard S. Sutton and\n   Andrew G. Barto](http://incompleteideas.net/book/ebook/)_\n-  [Epsilon-Greedy\n   Q-learning](https://www.baeldung.com/cs/epsilon-greedy-q-learning)_\n-  [Introduction to Reinforcement\n   Learning](https://gibberblot.github.io/rl-notes/index.html)_ by Tim\n   Miller (University of Melbourne)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}